{"cells":[{"cell_type":"code","source":["import torch\n","import os\n","import transformers\n","print(transformers.__version__)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VlpLtCINgNPY","executionInfo":{"status":"ok","timestamp":1733441379097,"user_tz":360,"elapsed":5779,"user":{"displayName":"Jiayi Xu","userId":"16580075947723802783"}},"outputId":"f7403084-a705-48a7-8afe-8520408a5c28"},"id":"VlpLtCINgNPY","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["4.46.3\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VwoqyTu314if","executionInfo":{"status":"ok","timestamp":1733441404897,"user_tz":360,"elapsed":25803,"user":{"displayName":"Jiayi Xu","userId":"16580075947723802783"}},"outputId":"eca878e3-4f7d-4b27-9a0c-ab51521c1142"},"id":"VwoqyTu314if","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# Define the path to the DS 5690 directory\n","BASE_DIR = '/content/drive/My Drive/Colab Notebooks/DS 5690/final_project/poetic-gpt-chinese'\n","model_path = os.path.join(BASE_DIR, 'save.model.pth')\n","# Ensure the directory exists\n","import os\n","os.makedirs(BASE_DIR, exist_ok=True)"],"metadata":{"id":"o8-BRvjI2HZF","executionInfo":{"status":"ok","timestamp":1733446775388,"user_tz":360,"elapsed":282,"user":{"displayName":"Jiayi Xu","userId":"16580075947723802783"}}},"id":"o8-BRvjI2HZF","execution_count":14,"outputs":[]},{"cell_type":"code","execution_count":4,"id":"ff223689","metadata":{"scrolled":false,"colab":{"base_uri":"https://localhost:8080/","height":415,"referenced_widgets":["e40023312080459cb865b8fca92cecfe","916a79fe39364609a2b90fd5f5021c5c","0d49928344ef474abf6ebfc5aa83060b","bd58e8dad06047a0b11555ecedd691f1","499d4db1ee8248a1a8a25238c7ae88d8","eb9faa893fa142fba2310253def0ac2f","215733bca9e34722abb8d842edd72dc7","7f71001f77b04f31922667d0e2e67926","ef46ad44ca5042e688063ea51cfc5795","018338a570364f71aa499798ea281a56","c156154b9b24418da25fb96e36399533","92eea51ef67644eb9ed9362dab572989","04224baa60374bf9b23a5e195e77dd68","854db283180e4114b73493a02b124ecb","9866926ea9ef4edead8b8e70b6e27dcd","a2400c9ba01f4c1c8869119cae98a25d","5fee88f4552c4093bd597a458df85e84","d2ebd47d17b24e9b8bfda9e2a3ed6b4e","c75e1539067843db85bb5a88e2d931ca","1c7207488fbb46e4930dd843c408a06c","68cf11a3631c479f97ae1c6dae0a1151","a7ef61767a034437b4078407f37ff51f","1765aa38f57444678d08e98c91785acf","d3b9d228867544229a255643f93a2ffe","0cd7110ded914090a8bf86f85996905b","0fb70d6d76d848958b32ee72708f66e5","ff8bd7978b134fd7b0847a7ce55442d8","64fb94c83ba8477b8151bf89366a6761","62074c8b785b464aa9955ad492588686","9923c6308b6946b698e1ec25c2d1f89c","e677d925f52b453b9d6491a64337a6fa","7cc3097118e646ec85fc558ecd0ba2f5","98997ceafa234a17aa204b571773a1d6","2201dcb87dd949819d993c5575570d9d","6b6225b65fed43f99cd32f12aad7d508","15a277a8765749ec9fafda5f0304716c","d6b2a9ca1a6449bba40925e3d0242b02","4e3af6bbc7da4cadace59fe7850c2e8b","1dc8e23b4b834cd28fb4210dde25c279","da4e1a38276340f7962641da5aa64db5","9341dfcb6eba411fad6b7e9c86e3d9d3","7041e0f3d0f84450b7414a8838199054","3864547d13cb42f2b64175d2e8623606","330218db988b4ff58359ce21543cde60"]},"id":"ff223689","executionInfo":{"status":"ok","timestamp":1733441412184,"user_tz":360,"elapsed":4585,"user":{"displayName":"Jiayi Xu","userId":"16580075947723802783"}},"outputId":"c30ee825-60c6-4f5a-8a5d-20427ec0bd4c"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/217 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e40023312080459cb865b8fca92cecfe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/577 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92eea51ef67644eb9ed9362dab572989"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1765aa38f57444678d08e98c91785acf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2201dcb87dd949819d993c5575570d9d"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["BertTokenizerFast(name_or_path='uer/gpt2-chinese-cluecorpussmall', vocab_size=21128, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}\n","{'input_ids': [[101, 3617, 1139, 3313, 1139, 1045, 6793, 6809, 117, 1283, 2255, 674, 2255, 1963, 4125, 1355, 119, 7557, 5640, 6624, 1403, 1921, 677, 3341, 117, 6852, 1316, 3655, 3215, 6628, 1316, 3299, 119, 102], [101, 4007, 4680, 3736, 2255, 1724, 3307, 2406, 117, 4635, 756, 7770, 1318, 2322, 4170, 3119, 119, 3189, 1726, 4896, 2512, 4959, 4541, 3312, 117, 7599, 6853, 4351, 1898, 1057, 2207, 3517, 119, 6823, 2273, 849, 2242, 3566, 4819, 5862, 117, 3171, 2359, 1963, 1383, 2779, 704, 3837, 119, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"]}],"source":["# Import the AutoTokenizer class from the transformers library\n","from transformers import AutoTokenizer\n","\n","# Load the tokenizer\n","# The 'AutoTokenizer.from_pretrained()' method is used to load a pre-trained tokenizer.\n","# Here, we are loading the tokenizer for the 'uer/gpt2-chinese-cluecorpussmall' model,\n","# which is a version of GPT-2 trained on a Chinese corpus (CLUECorpusSmall).\n","tokenizer = AutoTokenizer.from_pretrained('uer/gpt2-chinese-cluecorpussmall')\n","\n","# Print the details of the tokenizer object\n","# This will display information about the loaded tokenizer, such as its configuration and supported vocabularies.\n","print(tokenizer)\n","\n","# Tokenization demo\n","# The 'batch_encode_plus()' method tokenizes a batch of input sentences.\n","# Each sentence is split into smaller sub-word units (tokens) according to the tokenizer's vocabulary and rules.\n","# Here, two Chinese poems are passed as a list to the method for batch processing.\n","encoded = tokenizer.batch_encode_plus([\n","    # The first poem\n","    # Original (Chinese):\n","    '欲出未出光辣达,千山万山如火发.须臾走向天上来,逐却残星赶却月.',\n","    # Translation (English):\n","    # \"The light emerges but not yet fully, illuminating mountains as if ablaze.\n","    # In an instant, it rushes to the sky, chasing away the lingering stars and the moon.\"\n","\n","    # The second poem\n","    # Original (Chinese):\n","    '满目江山四望幽,白云高卷嶂烟收.日回禽影穿疏木,风递猿声入小楼.远岫似屏横碧落,断帆如叶截中流.',\n","    # Translation (English):\n","    # \"Vast landscapes stretch out serenely, as white clouds roll high and mountain mist recedes.\n","    # The sun reflects bird shadows through sparse trees, and wind carries the cries of apes to a small tower.\n","    # Distant peaks rise like screens in the azure sky, and broken sails drift like leaves across the midstream.\"\n","])\n","\n","print(encoded)\n"]},{"cell_type":"code","execution_count":5,"id":"3dc8dfdf","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3dc8dfdf","executionInfo":{"status":"ok","timestamp":1733441417187,"user_tz":360,"elapsed":5005,"user":{"displayName":"Jiayi Xu","userId":"16580075947723802783"}},"outputId":"1fb8889c-cb6d-4a5c-ba28-001341b343dd"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(304752, '欲出未出光辣达,千山万山如火发.须臾走向天上来,逐却残星赶却月.')"]},"metadata":{},"execution_count":5}],"source":["import torch\n","\n","# Define a custom Dataset class for handling a simple dataset\n","# This class inherits from 'torch.utils.data.Dataset', a PyTorch utility for datasets.\n","class Dataset(torch.utils.data.Dataset):\n","\n","    def __init__(self):\n","        \"\"\"\n","        Initialization method for the Dataset class.\n","        - This method loads a file named 'chinese_poems.txt',\n","          reads its content line by line, and stores the cleaned lines in a list.\n","        \"\"\"\n","        with open('/content/drive/My Drive/Colab Notebooks/DS 5690/final_project/poetic-gpt-chinese/chinese_poems.txt') as f:\n","            # Read all lines from the file\n","            lines = f.readlines()\n","        # Strip any leading/trailing whitespace from each line\n","        lines = [i.strip() for i in lines]\n","\n","        # Store the processed lines as the dataset\n","        self.lines = lines\n","\n","    def __len__(self):\n","        \"\"\"\n","        Returns the total number of samples in the dataset.\n","        - This allows PyTorch to determine the size of the dataset when iterating.\n","        \"\"\"\n","        return len(self.lines)\n","\n","    def __getitem__(self, i):\n","        \"\"\"\n","        Retrieves a single sample from the dataset.\n","        - `i` is the index of the sample to retrieve.\n","        - Returns the line of text corresponding to the index `i`.\n","        \"\"\"\n","        return self.lines[i]\n","\n","\n","# Instantiate the custom Dataset\n","dataset = Dataset()\n","\n","# Retrieve the size of the dataset and the first sample\n","len_dataset = len(dataset)   # Number of samples in the dataset\n","first_sample = dataset[0]    # The first line of the dataset\n","\n","# Display the results\n","len_dataset, first_sample"]},{"cell_type":"code","execution_count":6,"id":"750a592f","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"750a592f","executionInfo":{"status":"ok","timestamp":1733441473137,"user_tz":360,"elapsed":55952,"user":{"displayName":"Jiayi Xu","userId":"16580075947723802783"}},"outputId":"260e70ff-cab6-480d-b475-1e4176451a07"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(839587, '四时运灰琯，一夕变冬春。送寒馀雪尽，迎岁早梅新。')"]},"metadata":{},"execution_count":6}],"source":["import torch\n","import os\n","import pandas as pd\n","\n","# Define a custom Dataset class for handling a more complex dataset\n","class Dataset(torch.utils.data.Dataset):\n","\n","    def __init__(self):\n","        \"\"\"\n","        Initialization method for the Dataset class.\n","        - This method reads multiple CSV files from the 'more_datas' directory,\n","          processes the text data, and applies filtering and cleaning steps.\n","        \"\"\"\n","        data = []  # List to store data from all files\n","\n","        # Iterate through all files in the 'more_datas' directory\n","        for i in os.listdir('/content/drive/My Drive/Colab Notebooks/DS 5690/final_project/poetic-gpt-chinese/more_datas'):\n","            # Skip system-related files such as Jupyter notebook checkpoints\n","            if i == '.ipynb_checkpoints':\n","                continue\n","            # Read each CSV file into a pandas DataFrame and append it to the list\n","            data.append(pd.read_csv(f'/content/drive/My Drive/Colab Notebooks/DS 5690/final_project/poetic-gpt-chinese/more_datas/{i}'))\n","\n","        # Concatenate all the DataFrames into a single DataFrame\n","        data = pd.concat(data).reset_index()\n","\n","        # Select the column\n","        data = data['内容']\n","\n","        # Remove leading and trailing whitespace from each entry\n","        data = data.str.strip()\n","\n","        # Remove specific punctuation characters (e.g., 《》“”「」)\n","        # Use regex to identify and remove these characters\n","        data = data.str.replace('[《》“”「」]', '', regex=True)\n","\n","        # Apply regex-based filtering\n","        # Only keep entries that match the specified pattern:\n","        # ^[\\w，。？、！：；]+$ matches text containing Chinese characters, punctuation, and underscores.\n","        select = data.str.match('^[\\w，。？、！：；]+$', na=False)\n","        data = data[select]\n","\n","        # Normalize punctuation for consistency:\n","        # Replace '？！；' with '。' (convert question marks, exclamation points, and semicolons to periods)\n","        data = data.str.replace('[？！；]', '。', regex=True)\n","        # Replace '、：' with '，' (convert list markers and colons to commas)\n","        data = data.str.replace('[、：]', '，', regex=True)\n","\n","        # Store the cleaned and filtered data\n","        self.data = data\n","\n","    def __len__(self):\n","        \"\"\"\n","        Returns the total number of samples in the dataset.\n","        - This allows PyTorch utilities to determine the dataset size.\n","        \"\"\"\n","        return len(self.data)\n","\n","    def __getitem__(self, i):\n","        \"\"\"\n","        Retrieves a single sample from the dataset.\n","        - `i` is the index of the sample to retrieve.\n","        - Returns the text data (as a string) at the specified index.\n","        \"\"\"\n","        return self.data.iloc[i]\n","\n","\n","# Instantiate the custom Dataset\n","dataset = Dataset()\n","\n","# Retrieve the size of the dataset and the first sample\n","len_dataset = len(dataset)   # Number of samples in the dataset\n","first_sample = dataset[0]    # The first text entry in the dataset\n","\n","# Display the results\n","len_dataset, first_sample"]},{"cell_type":"code","execution_count":7,"id":"405095b6","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"405095b6","executionInfo":{"status":"ok","timestamp":1733441473138,"user_tz":360,"elapsed":4,"user":{"displayName":"Jiayi Xu","userId":"16580075947723802783"}},"outputId":"74245f30-96cf-4135-ed41-8384fdfde4f3"},"outputs":[{"output_type":"stream","name":"stdout","text":["input_ids torch.Size([8, 130])\n","token_type_ids torch.Size([8, 130])\n","attention_mask torch.Size([8, 130])\n","labels torch.Size([8, 130])\n"]},{"output_type":"execute_result","data":{"text/plain":["104948"]},"metadata":{},"execution_count":7}],"source":["def collate_fn(data):\n","    \"\"\"\n","    Custom collate function to process a batch of data for the DataLoader.\n","\n","    Parameters:\n","    - data (list): A batch of samples from the dataset (list of strings).\n","\n","    Returns:\n","    - data (dict): A dictionary containing tokenized data in PyTorch tensor format, including:\n","        - 'input_ids': Encoded token IDs for the input text.\n","        - 'attention_mask': Attention masks for the input text.\n","        - 'labels': A copy of the 'input_ids', which is often used as labels for language modeling tasks.\n","    \"\"\"\n","    # Tokenize the batch of text data\n","    data = tokenizer.batch_encode_plus(data,          # Input data (list of strings)\n","                                       padding=True,  # Pad sequences to the same length\n","                                       truncation=True,  # Truncate sequences to the maximum length\n","                                       max_length=512,   # Maximum token length per sequence\n","                                       return_tensors='pt')  # Return results as PyTorch tensors\n","\n","    # Clone 'input_ids' to use as labels\n","    # This is a typical practice in language modeling where the model predicts the next token\n","    data['labels'] = data['input_ids'].clone()\n","\n","    return data\n","\n","\n","# Create the DataLoader\n","# The DataLoader iterates over the dataset and provides batches of data for training/testing.\n","loader = torch.utils.data.DataLoader(\n","    dataset=dataset,       # The dataset object created earlier\n","    batch_size=8,          # Number of samples per batch\n","    collate_fn=collate_fn, # Custom collate function for processing batches\n","    shuffle=True,          # Shuffle the dataset at every epoch\n","    drop_last=True,        # Drop the last incomplete batch if the dataset size is not divisible by batch_size\n",")\n","\n","# Iterate over the DataLoader to get the first batch\n","for i, data in enumerate(loader):\n","    break  # Stop after processing the first batch\n","\n","# Print the keys and shapes of the tensors in the batch\n","for k, v in data.items():\n","    print(k, v.shape)  # k is the key (e.g., 'input_ids'), v.shape is the shape of the tensor\n","\n","# Get the total number of batches in the DataLoader\n","len_loader = len(loader)  # Number of batches\n","\n","len_loader"]},{"cell_type":"code","execution_count":8,"id":"f53f3c45","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/","height":103,"referenced_widgets":["17d1b792c6e04f1db222e8cebc81b778","375f9ecb32004e298e6fb3c02a33f35a","da3c5198e1a0429cb3b54050a3baf78d","4f67444df3614a7898f81bf09fa1add4","ac8d1459bfb443f9be0050fa32d04901","144942b05f0e440d9afbe2a9b5c6ec0b","f74a08037dc244878205bf53b8be64f6","3f59be421a5d436ba748f321b3fc5b58","46fa8ca4e372405295f0f0f4b229f401","0f74736332d04f68b20dc05279735d6a","d8a12e1bdf7242a0a0d79c3f2e17a058"]},"id":"f53f3c45","executionInfo":{"status":"ok","timestamp":1733441485913,"user_tz":360,"elapsed":12778,"user":{"displayName":"Jiayi Xu","userId":"16580075947723802783"}},"outputId":"e332f82f-63fb-4619-9bef-c1219316f3ee"},"outputs":[{"output_type":"display_data","data":{"text/plain":["pytorch_model.bin:   0%|          | 0.00/421M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17d1b792c6e04f1db222e8cebc81b778"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Number of model parameters (in tens of thousands): 10206.8736\n","Loss: tensor(9.4741)\n","Logits shape: torch.Size([8, 130, 21128])\n"]}],"source":["from transformers import AutoModelForCausalLM, GPT2Model\n","import torch\n","\n","# Load the pre-trained causal language model\n","# 'AutoModelForCausalLM' is used to load a model suitable for causal language modeling tasks (e.g., GPT-2).\n","# The model 'uer/gpt2-chinese-cluecorpussmall' is a GPT-2 variant fine-tuned on a Chinese corpus.\n","model = AutoModelForCausalLM.from_pretrained('uer/gpt2-chinese-cluecorpussmall')\n","\n","# Calculate the total number of model parameters\n","# Use 'model.parameters()' to access all parameters of the model.\n","# 'numel()' counts the total elements in each parameter tensor.\n","# The result is divided by 10,000 to display the parameter count in tens of thousands.\n","print(f\"Number of model parameters (in tens of thousands): {sum(p.numel() for p in model.parameters()) / 10000}\")\n","\n","# Perform a forward pass through the model without gradient computation\n","# 'torch.no_grad()' disables gradient tracking, which reduces memory usage during inference.\n","with torch.no_grad():\n","    # Perform the forward pass with the tokenized input data\n","    # 'data' is expected to be the batch created by the DataLoader, containing 'input_ids' and other necessary tensors.\n","    out = model(**data)\n","\n","# Extract and print the loss and the shape of the logits\n","# 'out' is a dictionary containing:\n","#   - 'loss': The loss value if labels are provided in the input.\n","#   - 'logits': The raw output predictions (scores before applying softmax) from the model.\n","print(\"Loss:\", out['loss'])\n","print(\"Logits shape:\", out['logits'].shape)\n"]},{"cell_type":"code","execution_count":40,"id":"bbd9b9ab","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bbd9b9ab","executionInfo":{"status":"ok","timestamp":1733448736990,"user_tz":360,"elapsed":2352,"user":{"displayName":"Jiayi Xu","userId":"16580075947723802783"}},"outputId":"ca4610fa-693f-4624-cfd2-0dbe4b693049"},"outputs":[{"output_type":"stream","name":"stdout","text":["0 [CLS] 秋 高 气 爽 人 ， 春 暖 新 衣 风 。 长 安 未 来 事 ， 时 间 流 向 长 。\n","1 [CLS] 秋 高 气 爽 宜 ， 冬 半 正 是 阳 。 万 里 风 沙 如 ， 水 清 秋 月 一 。\n","2 [CLS] 秋 高 气 爽 秋 ， 松 香 十 里 来 。 松 阴 欲 尽 夕 ， 松 枝 正 照 阳 。\n"]}],"source":["def generate(text, row, col):\n","    \"\"\"\n","    Generates text based on a given input seed using a causal language model.\n","\n","    Parameters:\n","    - text (str): The input seed text for generation.\n","    - row (int): Number of rows for the output structure.\n","    - col (int): Number of columns for the output structure.\n","\n","    The function uses a nested loop function (`generate_loop`) to iteratively generate tokens\n","    until the desired text length or structure is achieved.\n","    \"\"\"\n","\n","    def generate_loop(data):\n","        \"\"\"\n","        Performs the iterative generation loop to produce tokens.\n","\n","        Parameters:\n","        - data (dict): A dictionary containing the tokenized input data.\n","\n","        Returns:\n","        - data (dict): Updated data with additional generated tokens.\n","        \"\"\"\n","        with torch.no_grad():\n","            # Perform a forward pass through the model\n","            out = model(**data)\n","\n","        # Extract logits (predictions) from the model's output\n","        out = out['logits']  # Shape: [batch_size, sequence_length, vocab_size]\n","        out = out[:, -1]     # Take the last token's logits. Shape: [batch_size, vocab_size]\n","\n","        # Filter tokens to keep only the top 50 probabilities\n","        topk_value = torch.topk(out, 50).values  # Get the top 50 values\n","        topk_value = topk_value[:, -1].unsqueeze(dim=1)  # Take the 50th value (threshold)\n","\n","        # Mask out logits below the threshold (assign them negative infinity)\n","        out = out.masked_fill(out < topk_value, -float('inf'))\n","\n","        # Prevent generation of special symbols\n","        out[:, tokenizer.sep_token_id] = -float('inf')\n","        out[:, tokenizer.unk_token_id] = -float('inf')\n","        out[:, tokenizer.pad_token_id] = -float('inf')\n","\n","        # Prevent generation of specific punctuation symbols\n","        for i in '，。':\n","            out[:, tokenizer.get_vocab()[i]] = -float('inf')\n","\n","        # Sample the next token based on probabilities (no replacement)\n","        out = out.softmax(dim=1)         # Convert logits to probabilities\n","        out = out.multinomial(num_samples=1)  # Sample one token per batch\n","\n","        # Enforce punctuation at certain positions based on the column structure\n","        c = data['input_ids'].shape[1] / (col + 1)  # Calculate the position\n","        if c % 1 == 0:  # If it's at the end of a row\n","            if c % 2 == 0:  # Alternate between '。' and '，'\n","                out[:, 0] = tokenizer.get_vocab()['。']\n","            else:\n","                out[:, 0] = tokenizer.get_vocab()['，']\n","\n","        # Append the new token to the input sequence\n","        data['input_ids'] = torch.cat([data['input_ids'], out], dim=1)\n","\n","        # Update attention and token type masks\n","        data['attention_mask'] = torch.ones_like(data['input_ids'])\n","        data['token_type_ids'] = torch.zeros_like(data['input_ids'])\n","\n","        # Clone the updated input as labels for loss calculation\n","        data['labels'] = data['input_ids'].clone()\n","\n","        # Check if the generated sequence has reached the target length\n","        if data['input_ids'].shape[1] >= row * col + row + 1:\n","            return data\n","\n","        # Recursively call the function until the condition is met\n","        return generate_loop(data)\n","\n","    # Prepare the input data (repeat the seed text 3 times for generation)\n","    data = tokenizer.batch_encode_plus([text] * 3, return_tensors='pt')\n","    data['input_ids'] = data['input_ids'][:, :-1]  # Remove the end token for initial input\n","    data['attention_mask'] = torch.ones_like(data['input_ids'])  # Initialize attention mask\n","    data['token_type_ids'] = torch.zeros_like(data['input_ids'])  # Initialize token type IDs\n","    data['labels'] = data['input_ids'].clone()  # Clone input IDs as labels\n","\n","    # Start the recursive generation loop\n","    data = generate_loop(data)\n","\n","    # Decode and print the generated sequences\n","    for i in range(3):\n","        print(i, tokenizer.decode(data['input_ids'][i]))\n","\n","\n","# Generate text using the function\n","generate('秋高气爽', row=4, col=5)\n"]},{"cell_type":"code","execution_count":10,"id":"fb0e6ed9","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"fb0e6ed9","outputId":"ea62810c-fbbd-4eb9-a241-91893dca72c0","executionInfo":{"status":"ok","timestamp":1733446652143,"user_tz":360,"elapsed":5164876,"user":{"displayName":"Jiayi Xu","userId":"16580075947723802783"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","<ipython-input-10-d113e655c0ba>:31: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = GradScaler()\n","<ipython-input-10-d113e655c0ba>:49: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with autocast():\n"]},{"output_type":"stream","name":"stdout","text":["Step 0/104948: Loss=9.5048, Estimated time left: 2 days, 5:17:30\n","Step 0: Loss=9.5048, LR=4.999952e-05, Accuracy=0.0692\n","Step 100/104948: Loss=8.7636, Estimated time left: 2:06:31\n","Step 200/104948: Loss=5.7700, Estimated time left: 1:46:12\n","Step 300/104948: Loss=4.5879, Estimated time left: 1:38:54\n","Step 400/104948: Loss=2.5003, Estimated time left: 1:35:03\n","Step 500/104948: Loss=3.1449, Estimated time left: 1:33:08\n","Step 600/104948: Loss=2.7162, Estimated time left: 1:31:33\n","Step 700/104948: Loss=2.9393, Estimated time left: 1:30:11\n","Step 800/104948: Loss=2.8048, Estimated time left: 1:29:18\n","Step 900/104948: Loss=2.4995, Estimated time left: 1:28:31\n","Step 1000/104948: Loss=2.3800, Estimated time left: 1:28:02\n","Step 1000: Loss=2.3800, LR=4.952310e-05, Accuracy=0.1806\n","Step 1100/104948: Loss=2.3934, Estimated time left: 1:27:32\n","Step 1200/104948: Loss=2.8190, Estimated time left: 1:27:14\n","Step 1300/104948: Loss=2.4244, Estimated time left: 1:26:57\n","Step 1400/104948: Loss=2.8616, Estimated time left: 1:26:20\n","Step 1500/104948: Loss=1.3810, Estimated time left: 1:25:55\n","Step 1600/104948: Loss=3.8148, Estimated time left: 1:25:37\n","Step 1700/104948: Loss=2.6356, Estimated time left: 1:25:12\n","Step 1800/104948: Loss=3.3485, Estimated time left: 1:24:57\n","Step 1900/104948: Loss=3.1923, Estimated time left: 1:24:46\n","Step 2000/104948: Loss=4.2088, Estimated time left: 1:24:31\n","Step 2000: Loss=4.2088, LR=4.904667e-05, Accuracy=0.1888\n","Step 2100/104948: Loss=3.4310, Estimated time left: 1:24:24\n","Step 2200/104948: Loss=3.5726, Estimated time left: 1:24:12\n","Step 2300/104948: Loss=3.1276, Estimated time left: 1:24:00\n","Step 2400/104948: Loss=2.1511, Estimated time left: 1:23:51\n","Step 2500/104948: Loss=2.5219, Estimated time left: 1:23:44\n","Step 2600/104948: Loss=2.8021, Estimated time left: 1:23:39\n","Step 2700/104948: Loss=2.9381, Estimated time left: 1:23:31\n","Step 2800/104948: Loss=1.3514, Estimated time left: 1:23:31\n","Step 2900/104948: Loss=2.4851, Estimated time left: 1:23:26\n","Step 3000/104948: Loss=2.9040, Estimated time left: 1:23:20\n","Step 3000: Loss=2.9040, LR=4.857024e-05, Accuracy=0.1833\n","Step 3100/104948: Loss=3.1397, Estimated time left: 1:23:15\n","Step 3200/104948: Loss=1.5042, Estimated time left: 1:23:08\n","Step 3300/104948: Loss=2.5437, Estimated time left: 1:23:04\n","Step 3400/104948: Loss=2.2995, Estimated time left: 1:22:54\n","Step 3500/104948: Loss=3.7885, Estimated time left: 1:22:46\n","Step 3600/104948: Loss=2.7134, Estimated time left: 1:22:36\n","Step 3700/104948: Loss=1.2377, Estimated time left: 1:22:27\n","Step 3800/104948: Loss=1.1762, Estimated time left: 1:22:19\n","Step 3900/104948: Loss=2.1664, Estimated time left: 1:22:13\n","Step 4000/104948: Loss=2.2588, Estimated time left: 1:22:05\n","Step 4000: Loss=2.2588, LR=4.809382e-05, Accuracy=0.2207\n","Step 4100/104948: Loss=2.6497, Estimated time left: 1:22:00\n","Step 4200/104948: Loss=2.3927, Estimated time left: 1:21:56\n","Step 4300/104948: Loss=2.6182, Estimated time left: 1:21:49\n","Step 4400/104948: Loss=4.1730, Estimated time left: 1:21:44\n","Step 4500/104948: Loss=4.0473, Estimated time left: 1:21:39\n","Step 4600/104948: Loss=3.7077, Estimated time left: 1:21:34\n","Step 4700/104948: Loss=1.7963, Estimated time left: 1:21:32\n","Step 4800/104948: Loss=1.7123, Estimated time left: 1:21:23\n","Step 4900/104948: Loss=3.4153, Estimated time left: 1:21:17\n","Step 5000/104948: Loss=3.8118, Estimated time left: 1:21:10\n","Step 5000: Loss=3.8118, LR=4.761739e-05, Accuracy=0.2038\n","Step 5100/104948: Loss=3.8281, Estimated time left: 1:21:03\n","Step 5200/104948: Loss=2.7217, Estimated time left: 1:20:59\n","Step 5300/104948: Loss=1.8639, Estimated time left: 1:20:55\n","Step 5400/104948: Loss=2.9504, Estimated time left: 1:20:53\n","Step 5500/104948: Loss=4.5641, Estimated time left: 1:20:47\n","Step 5600/104948: Loss=2.3695, Estimated time left: 1:20:41\n","Step 5700/104948: Loss=2.8409, Estimated time left: 1:20:38\n","Step 5800/104948: Loss=2.8823, Estimated time left: 1:20:32\n","Step 5900/104948: Loss=3.4799, Estimated time left: 1:20:26\n","Step 6000/104948: Loss=2.0769, Estimated time left: 1:20:21\n","Step 6000: Loss=2.0769, LR=4.714097e-05, Accuracy=0.2124\n","Step 6100/104948: Loss=3.1543, Estimated time left: 1:20:16\n","Step 6200/104948: Loss=3.0379, Estimated time left: 1:20:12\n","Step 6300/104948: Loss=1.5717, Estimated time left: 1:20:10\n","Step 6400/104948: Loss=2.5277, Estimated time left: 1:20:04\n","Step 6500/104948: Loss=2.3710, Estimated time left: 1:19:58\n","Step 6600/104948: Loss=2.5962, Estimated time left: 1:19:52\n","Step 6700/104948: Loss=3.9996, Estimated time left: 1:19:47\n","Step 6800/104948: Loss=1.8624, Estimated time left: 1:19:43\n","Step 6900/104948: Loss=3.3773, Estimated time left: 1:19:37\n","Step 7000/104948: Loss=2.2361, Estimated time left: 1:19:34\n","Step 7000: Loss=2.2361, LR=4.666454e-05, Accuracy=0.2051\n","Step 7100/104948: Loss=1.3282, Estimated time left: 1:19:28\n","Step 7200/104948: Loss=2.5222, Estimated time left: 1:19:24\n","Step 7300/104948: Loss=1.7247, Estimated time left: 1:19:18\n","Step 7400/104948: Loss=2.8230, Estimated time left: 1:19:13\n","Step 7500/104948: Loss=2.5515, Estimated time left: 1:19:07\n","Step 7600/104948: Loss=2.9168, Estimated time left: 1:19:02\n","Step 7700/104948: Loss=3.5578, Estimated time left: 1:18:56\n","Step 7800/104948: Loss=3.9499, Estimated time left: 1:18:52\n","Step 7900/104948: Loss=1.5248, Estimated time left: 1:18:46\n","Step 8000/104948: Loss=1.1335, Estimated time left: 1:18:41\n","Step 8000: Loss=1.1335, LR=4.618811e-05, Accuracy=0.1891\n","Step 8100/104948: Loss=1.4780, Estimated time left: 1:18:37\n","Step 8200/104948: Loss=2.2190, Estimated time left: 1:18:31\n","Step 8300/104948: Loss=3.5243, Estimated time left: 1:18:26\n","Step 8400/104948: Loss=2.0658, Estimated time left: 1:18:22\n","Step 8500/104948: Loss=3.5727, Estimated time left: 1:18:17\n","Step 8600/104948: Loss=2.6051, Estimated time left: 1:18:12\n","Step 8700/104948: Loss=3.7629, Estimated time left: 1:18:08\n","Step 8800/104948: Loss=3.1842, Estimated time left: 1:18:02\n","Step 8900/104948: Loss=2.5078, Estimated time left: 1:17:59\n","Step 9000/104948: Loss=1.6119, Estimated time left: 1:17:55\n","Step 9000: Loss=1.6119, LR=4.571169e-05, Accuracy=0.1928\n","Step 9100/104948: Loss=3.5937, Estimated time left: 1:17:50\n","Step 9200/104948: Loss=3.8335, Estimated time left: 1:17:45\n","Step 9300/104948: Loss=1.1527, Estimated time left: 1:17:41\n","Step 9400/104948: Loss=1.6858, Estimated time left: 1:17:37\n","Step 9500/104948: Loss=3.3463, Estimated time left: 1:17:31\n","Step 9600/104948: Loss=1.3515, Estimated time left: 1:17:27\n","Step 9700/104948: Loss=3.0795, Estimated time left: 1:17:23\n","Step 9800/104948: Loss=3.2238, Estimated time left: 1:17:19\n","Step 9900/104948: Loss=3.5875, Estimated time left: 1:17:14\n","Step 10000/104948: Loss=2.4738, Estimated time left: 1:17:09\n","Step 10000: Loss=2.4738, LR=4.523526e-05, Accuracy=0.2114\n","Step 10100/104948: Loss=2.3421, Estimated time left: 1:17:03\n","Step 10200/104948: Loss=2.2414, Estimated time left: 1:16:58\n","Step 10300/104948: Loss=3.9408, Estimated time left: 1:16:53\n","Step 10400/104948: Loss=2.9564, Estimated time left: 1:16:48\n","Step 10500/104948: Loss=1.7897, Estimated time left: 1:16:42\n","Step 10600/104948: Loss=2.0949, Estimated time left: 1:16:37\n","Step 10700/104948: Loss=2.3646, Estimated time left: 1:16:33\n","Step 10800/104948: Loss=2.0707, Estimated time left: 1:16:27\n","Step 10900/104948: Loss=2.4871, Estimated time left: 1:16:22\n","Step 11000/104948: Loss=2.8334, Estimated time left: 1:16:17\n","Step 11000: Loss=2.8334, LR=4.475883e-05, Accuracy=0.1925\n","Step 11100/104948: Loss=2.6489, Estimated time left: 1:16:12\n","Step 11200/104948: Loss=1.1062, Estimated time left: 1:16:08\n","Step 11300/104948: Loss=2.2558, Estimated time left: 1:16:03\n","Step 11400/104948: Loss=4.4036, Estimated time left: 1:15:59\n","Step 11500/104948: Loss=1.8746, Estimated time left: 1:15:54\n","Step 11600/104948: Loss=1.5773, Estimated time left: 1:15:48\n","Step 11700/104948: Loss=1.9072, Estimated time left: 1:15:44\n","Step 11800/104948: Loss=2.3282, Estimated time left: 1:15:38\n","Step 11900/104948: Loss=4.8007, Estimated time left: 1:15:32\n","Step 12000/104948: Loss=2.1226, Estimated time left: 1:15:27\n","Step 12000: Loss=2.1226, LR=4.428241e-05, Accuracy=0.1988\n","Step 12100/104948: Loss=3.1256, Estimated time left: 1:15:22\n","Step 12200/104948: Loss=3.6610, Estimated time left: 1:15:17\n","Step 12300/104948: Loss=3.5282, Estimated time left: 1:15:12\n","Step 12400/104948: Loss=1.3693, Estimated time left: 1:15:06\n","Step 12500/104948: Loss=2.0098, Estimated time left: 1:15:02\n","Step 12600/104948: Loss=2.7263, Estimated time left: 1:14:58\n","Step 12700/104948: Loss=3.2434, Estimated time left: 1:14:53\n","Step 12800/104948: Loss=2.5885, Estimated time left: 1:14:48\n","Step 12900/104948: Loss=3.3096, Estimated time left: 1:14:43\n","Step 13000/104948: Loss=3.2848, Estimated time left: 1:14:38\n","Step 13000: Loss=3.2848, LR=4.380598e-05, Accuracy=0.1836\n","Step 13100/104948: Loss=1.8717, Estimated time left: 1:14:33\n","Step 13200/104948: Loss=1.1473, Estimated time left: 1:14:28\n","Step 13300/104948: Loss=2.1080, Estimated time left: 1:14:24\n","Step 13400/104948: Loss=2.6422, Estimated time left: 1:14:18\n","Step 13500/104948: Loss=1.5879, Estimated time left: 1:14:12\n","Step 13600/104948: Loss=2.7913, Estimated time left: 1:14:07\n","Step 13700/104948: Loss=1.2849, Estimated time left: 1:14:02\n","Step 13800/104948: Loss=2.2562, Estimated time left: 1:13:58\n","Step 13900/104948: Loss=4.2857, Estimated time left: 1:13:53\n","Step 14000/104948: Loss=2.5973, Estimated time left: 1:13:48\n","Step 14000: Loss=2.5973, LR=4.332955e-05, Accuracy=0.2013\n","Step 14100/104948: Loss=2.1939, Estimated time left: 1:13:43\n","Step 14200/104948: Loss=1.8232, Estimated time left: 1:13:37\n","Step 14300/104948: Loss=2.0661, Estimated time left: 1:13:32\n","Step 14400/104948: Loss=2.9209, Estimated time left: 1:13:28\n","Step 14500/104948: Loss=2.2100, Estimated time left: 1:13:23\n","Step 14600/104948: Loss=3.5462, Estimated time left: 1:13:17\n","Step 14700/104948: Loss=2.3673, Estimated time left: 1:13:12\n","Step 14800/104948: Loss=3.9113, Estimated time left: 1:13:07\n","Step 14900/104948: Loss=2.8311, Estimated time left: 1:13:02\n","Step 15000/104948: Loss=2.8604, Estimated time left: 1:12:57\n","Step 15000: Loss=2.8604, LR=4.285313e-05, Accuracy=0.2094\n","Step 15100/104948: Loss=2.4824, Estimated time left: 1:12:52\n","Step 15200/104948: Loss=4.8726, Estimated time left: 1:12:47\n","Step 15300/104948: Loss=4.6234, Estimated time left: 1:12:42\n","Step 15400/104948: Loss=3.6470, Estimated time left: 1:12:38\n","Step 15500/104948: Loss=2.3151, Estimated time left: 1:12:33\n","Step 15600/104948: Loss=2.6091, Estimated time left: 1:12:28\n","Step 15700/104948: Loss=2.9417, Estimated time left: 1:12:23\n","Step 15800/104948: Loss=1.5677, Estimated time left: 1:12:19\n","Step 15900/104948: Loss=3.2972, Estimated time left: 1:12:14\n","Step 16000/104948: Loss=2.1532, Estimated time left: 1:12:09\n","Step 16000: Loss=2.1532, LR=4.237670e-05, Accuracy=0.1939\n","Step 16100/104948: Loss=2.1327, Estimated time left: 1:12:03\n","Step 16200/104948: Loss=3.1909, Estimated time left: 1:11:58\n","Step 16300/104948: Loss=2.7648, Estimated time left: 1:11:53\n","Step 16400/104948: Loss=2.0850, Estimated time left: 1:11:47\n","Step 16500/104948: Loss=2.4930, Estimated time left: 1:11:43\n","Step 16600/104948: Loss=3.9790, Estimated time left: 1:11:37\n","Step 16700/104948: Loss=2.2894, Estimated time left: 1:11:33\n","Step 16800/104948: Loss=4.0206, Estimated time left: 1:11:28\n","Step 16900/104948: Loss=1.5094, Estimated time left: 1:11:23\n","Step 17000/104948: Loss=2.5633, Estimated time left: 1:11:18\n","Step 17000: Loss=2.5633, LR=4.190027e-05, Accuracy=0.2103\n","Step 17100/104948: Loss=3.9509, Estimated time left: 1:11:14\n","Step 17200/104948: Loss=4.0249, Estimated time left: 1:11:08\n","Step 17300/104948: Loss=1.9153, Estimated time left: 1:11:04\n","Step 17400/104948: Loss=3.4520, Estimated time left: 1:10:59\n","Step 17500/104948: Loss=2.6233, Estimated time left: 1:10:54\n","Step 17600/104948: Loss=1.5286, Estimated time left: 1:10:49\n","Step 17700/104948: Loss=2.8011, Estimated time left: 1:10:45\n","Step 17800/104948: Loss=2.7883, Estimated time left: 1:10:40\n","Step 17900/104948: Loss=3.0955, Estimated time left: 1:10:35\n","Step 18000/104948: Loss=3.8253, Estimated time left: 1:10:30\n","Step 18000: Loss=3.8253, LR=4.142385e-05, Accuracy=0.2149\n","Step 18100/104948: Loss=2.3628, Estimated time left: 1:10:25\n","Step 18200/104948: Loss=2.7653, Estimated time left: 1:10:20\n","Step 18300/104948: Loss=2.1006, Estimated time left: 1:10:16\n","Step 18400/104948: Loss=2.5333, Estimated time left: 1:10:10\n","Step 18500/104948: Loss=2.6158, Estimated time left: 1:10:05\n","Step 18600/104948: Loss=2.6615, Estimated time left: 1:10:00\n","Step 18700/104948: Loss=1.7298, Estimated time left: 1:09:56\n","Step 18800/104948: Loss=4.6172, Estimated time left: 1:09:51\n","Step 18900/104948: Loss=2.1962, Estimated time left: 1:09:47\n","Step 19000/104948: Loss=2.9145, Estimated time left: 1:09:42\n","Step 19000: Loss=2.9145, LR=4.094742e-05, Accuracy=0.2091\n","Step 19100/104948: Loss=4.6072, Estimated time left: 1:09:37\n","Step 19200/104948: Loss=2.6906, Estimated time left: 1:09:32\n","Step 19300/104948: Loss=3.2524, Estimated time left: 1:09:27\n","Step 19400/104948: Loss=2.3328, Estimated time left: 1:09:23\n","Step 19500/104948: Loss=3.4350, Estimated time left: 1:09:18\n","Step 19600/104948: Loss=1.9934, Estimated time left: 1:09:14\n","Step 19700/104948: Loss=2.0713, Estimated time left: 1:09:09\n","Step 19800/104948: Loss=2.4586, Estimated time left: 1:09:05\n","Step 19900/104948: Loss=1.4874, Estimated time left: 1:09:00\n","Step 20000/104948: Loss=2.6651, Estimated time left: 1:08:54\n","Step 20000: Loss=2.6651, LR=4.047100e-05, Accuracy=0.1736\n","Step 20100/104948: Loss=3.0778, Estimated time left: 1:08:50\n","Step 20200/104948: Loss=4.9632, Estimated time left: 1:08:44\n","Step 20300/104948: Loss=3.7688, Estimated time left: 1:08:39\n","Step 20400/104948: Loss=1.7405, Estimated time left: 1:08:34\n","Step 20500/104948: Loss=2.8633, Estimated time left: 1:08:29\n","Step 20600/104948: Loss=2.6755, Estimated time left: 1:08:24\n","Step 20700/104948: Loss=2.4234, Estimated time left: 1:08:19\n","Step 20800/104948: Loss=2.3765, Estimated time left: 1:08:14\n","Step 20900/104948: Loss=3.1338, Estimated time left: 1:08:09\n","Step 21000/104948: Loss=3.1885, Estimated time left: 1:08:04\n","Step 21000: Loss=3.1885, LR=3.999457e-05, Accuracy=0.1866\n","Step 21100/104948: Loss=4.0253, Estimated time left: 1:07:59\n","Step 21200/104948: Loss=2.1770, Estimated time left: 1:07:54\n","Step 21300/104948: Loss=2.6544, Estimated time left: 1:07:49\n","Step 21400/104948: Loss=2.5960, Estimated time left: 1:07:44\n","Step 21500/104948: Loss=2.4310, Estimated time left: 1:07:39\n","Step 21600/104948: Loss=3.0446, Estimated time left: 1:07:35\n","Step 21700/104948: Loss=2.2661, Estimated time left: 1:07:30\n","Step 21800/104948: Loss=3.1741, Estimated time left: 1:07:25\n","Step 21900/104948: Loss=2.8854, Estimated time left: 1:07:20\n","Step 22000/104948: Loss=2.7198, Estimated time left: 1:07:15\n","Step 22000: Loss=2.7198, LR=3.951814e-05, Accuracy=0.1949\n","Step 22100/104948: Loss=1.6508, Estimated time left: 1:07:10\n","Step 22200/104948: Loss=2.0585, Estimated time left: 1:07:05\n","Step 22300/104948: Loss=4.4086, Estimated time left: 1:07:01\n","Step 22400/104948: Loss=1.4515, Estimated time left: 1:06:56\n","Step 22500/104948: Loss=3.1088, Estimated time left: 1:06:51\n","Step 22600/104948: Loss=1.9761, Estimated time left: 1:06:46\n","Step 22700/104948: Loss=2.2445, Estimated time left: 1:06:41\n","Step 22800/104948: Loss=2.1676, Estimated time left: 1:06:36\n","Step 22900/104948: Loss=3.5816, Estimated time left: 1:06:31\n","Step 23000/104948: Loss=3.0562, Estimated time left: 1:06:26\n","Step 23000: Loss=3.0562, LR=3.904172e-05, Accuracy=0.2068\n","Step 23100/104948: Loss=2.4053, Estimated time left: 1:06:21\n","Step 23200/104948: Loss=2.9491, Estimated time left: 1:06:17\n","Step 23300/104948: Loss=1.7811, Estimated time left: 1:06:12\n","Step 23400/104948: Loss=3.3548, Estimated time left: 1:06:07\n","Step 23500/104948: Loss=2.1034, Estimated time left: 1:06:02\n","Step 23600/104948: Loss=1.9804, Estimated time left: 1:05:57\n","Step 23700/104948: Loss=4.4549, Estimated time left: 1:05:52\n","Step 23800/104948: Loss=1.8580, Estimated time left: 1:05:47\n","Step 23900/104948: Loss=3.8998, Estimated time left: 1:05:43\n","Step 24000/104948: Loss=2.7843, Estimated time left: 1:05:38\n","Step 24000: Loss=2.7843, LR=3.856529e-05, Accuracy=0.1885\n","Step 24100/104948: Loss=2.7862, Estimated time left: 1:05:33\n","Step 24200/104948: Loss=3.0725, Estimated time left: 1:05:29\n","Step 24300/104948: Loss=2.4777, Estimated time left: 1:05:24\n","Step 24400/104948: Loss=3.5200, Estimated time left: 1:05:19\n","Step 24500/104948: Loss=2.2836, Estimated time left: 1:05:15\n","Step 24600/104948: Loss=2.4355, Estimated time left: 1:05:10\n","Step 24700/104948: Loss=2.8617, Estimated time left: 1:05:05\n","Step 24800/104948: Loss=2.6603, Estimated time left: 1:05:00\n","Step 24900/104948: Loss=1.5124, Estimated time left: 1:04:56\n","Step 25000/104948: Loss=3.1456, Estimated time left: 1:04:50\n","Step 25000: Loss=3.1456, LR=3.808886e-05, Accuracy=0.2284\n","Step 25100/104948: Loss=1.2080, Estimated time left: 1:04:46\n","Step 25200/104948: Loss=2.3037, Estimated time left: 1:04:41\n","Step 25300/104948: Loss=2.5672, Estimated time left: 1:04:35\n","Step 25400/104948: Loss=4.1098, Estimated time left: 1:04:31\n","Step 25500/104948: Loss=2.4998, Estimated time left: 1:04:26\n","Step 25600/104948: Loss=2.1096, Estimated time left: 1:04:22\n","Step 25700/104948: Loss=1.7650, Estimated time left: 1:04:17\n","Step 25800/104948: Loss=2.5604, Estimated time left: 1:04:11\n","Step 25900/104948: Loss=1.9035, Estimated time left: 1:04:07\n","Step 26000/104948: Loss=2.0080, Estimated time left: 1:04:02\n","Step 26000: Loss=2.0080, LR=3.761244e-05, Accuracy=0.1965\n","Step 26100/104948: Loss=2.6307, Estimated time left: 1:03:57\n","Step 26200/104948: Loss=2.2589, Estimated time left: 1:03:52\n","Step 26300/104948: Loss=2.0472, Estimated time left: 1:03:47\n","Step 26400/104948: Loss=3.2955, Estimated time left: 1:03:42\n","Step 26500/104948: Loss=1.6715, Estimated time left: 1:03:37\n","Step 26600/104948: Loss=2.8500, Estimated time left: 1:03:33\n","Step 26700/104948: Loss=1.7419, Estimated time left: 1:03:28\n","Step 26800/104948: Loss=3.4968, Estimated time left: 1:03:23\n","Step 26900/104948: Loss=3.5617, Estimated time left: 1:03:18\n","Step 27000/104948: Loss=1.8732, Estimated time left: 1:03:13\n","Step 27000: Loss=1.8732, LR=3.713601e-05, Accuracy=0.2042\n","Step 27100/104948: Loss=2.6964, Estimated time left: 1:03:08\n","Step 27200/104948: Loss=2.8983, Estimated time left: 1:03:04\n","Step 27300/104948: Loss=3.3736, Estimated time left: 1:02:59\n","Step 27400/104948: Loss=2.2641, Estimated time left: 1:02:54\n","Step 27500/104948: Loss=2.1951, Estimated time left: 1:02:49\n","Step 27600/104948: Loss=2.7610, Estimated time left: 1:02:44\n","Step 27700/104948: Loss=3.3664, Estimated time left: 1:02:39\n","Step 27800/104948: Loss=2.6823, Estimated time left: 1:02:35\n","Step 27900/104948: Loss=2.2490, Estimated time left: 1:02:30\n","Step 28000/104948: Loss=2.0601, Estimated time left: 1:02:25\n","Step 28000: Loss=2.0601, LR=3.665958e-05, Accuracy=0.2123\n","Step 28100/104948: Loss=4.0436, Estimated time left: 1:02:21\n","Step 28200/104948: Loss=2.3833, Estimated time left: 1:02:16\n","Step 28300/104948: Loss=2.4428, Estimated time left: 1:02:11\n","Step 28400/104948: Loss=2.7574, Estimated time left: 1:02:06\n","Step 28500/104948: Loss=2.7444, Estimated time left: 1:02:02\n","Step 28600/104948: Loss=2.3623, Estimated time left: 1:01:57\n","Step 28700/104948: Loss=2.6130, Estimated time left: 1:01:52\n","Step 28800/104948: Loss=3.0815, Estimated time left: 1:01:47\n","Step 28900/104948: Loss=3.5375, Estimated time left: 1:01:42\n","Step 29000/104948: Loss=1.9199, Estimated time left: 1:01:37\n","Step 29000: Loss=1.9199, LR=3.618316e-05, Accuracy=0.1910\n","Step 29100/104948: Loss=2.9157, Estimated time left: 1:01:32\n","Step 29200/104948: Loss=3.4227, Estimated time left: 1:01:28\n","Step 29300/104948: Loss=1.4565, Estimated time left: 1:01:23\n","Step 29400/104948: Loss=4.6280, Estimated time left: 1:01:18\n","Step 29500/104948: Loss=4.5535, Estimated time left: 1:01:13\n","Step 29600/104948: Loss=4.9004, Estimated time left: 1:01:09\n","Step 29700/104948: Loss=4.0381, Estimated time left: 1:01:04\n","Step 29800/104948: Loss=4.6703, Estimated time left: 1:00:58\n","Step 29900/104948: Loss=2.9607, Estimated time left: 1:00:54\n","Step 30000/104948: Loss=2.1051, Estimated time left: 1:00:49\n","Step 30000: Loss=2.1051, LR=3.570673e-05, Accuracy=0.1972\n","Step 30100/104948: Loss=2.7443, Estimated time left: 1:00:44\n","Step 30200/104948: Loss=2.5231, Estimated time left: 1:00:39\n","Step 30300/104948: Loss=2.8567, Estimated time left: 1:00:35\n","Step 30400/104948: Loss=3.9818, Estimated time left: 1:00:30\n","Step 30500/104948: Loss=2.6566, Estimated time left: 1:00:25\n","Step 30600/104948: Loss=2.2117, Estimated time left: 1:00:20\n","Step 30700/104948: Loss=4.7229, Estimated time left: 1:00:15\n","Step 30800/104948: Loss=2.0256, Estimated time left: 1:00:10\n","Step 30900/104948: Loss=1.9814, Estimated time left: 1:00:06\n","Step 31000/104948: Loss=3.2724, Estimated time left: 1:00:01\n","Step 31000: Loss=3.2724, LR=3.523030e-05, Accuracy=0.1992\n","Step 31100/104948: Loss=2.0582, Estimated time left: 0:59:56\n","Step 31200/104948: Loss=2.7639, Estimated time left: 0:59:52\n","Step 31300/104948: Loss=3.3974, Estimated time left: 0:59:47\n","Step 31400/104948: Loss=2.1356, Estimated time left: 0:59:42\n","Step 31500/104948: Loss=2.0943, Estimated time left: 0:59:37\n","Step 31600/104948: Loss=1.3810, Estimated time left: 0:59:33\n","Step 31700/104948: Loss=2.3251, Estimated time left: 0:59:28\n","Step 31800/104948: Loss=2.3733, Estimated time left: 0:59:23\n","Step 31900/104948: Loss=1.7439, Estimated time left: 0:59:19\n","Step 32000/104948: Loss=1.9431, Estimated time left: 0:59:13\n","Step 32000: Loss=1.9431, LR=3.475388e-05, Accuracy=0.1837\n","Step 32100/104948: Loss=2.5035, Estimated time left: 0:59:09\n","Step 32200/104948: Loss=1.4160, Estimated time left: 0:59:04\n","Step 32300/104948: Loss=2.8903, Estimated time left: 0:58:59\n","Step 32400/104948: Loss=2.0457, Estimated time left: 0:58:54\n","Step 32500/104948: Loss=3.8104, Estimated time left: 0:58:50\n","Step 32600/104948: Loss=3.6366, Estimated time left: 0:58:45\n","Step 32700/104948: Loss=3.1982, Estimated time left: 0:58:41\n","Step 32800/104948: Loss=1.9641, Estimated time left: 0:58:36\n","Step 32900/104948: Loss=1.7026, Estimated time left: 0:58:31\n","Step 33000/104948: Loss=3.8531, Estimated time left: 0:58:26\n","Step 33000: Loss=3.8531, LR=3.427745e-05, Accuracy=0.2214\n","Step 33100/104948: Loss=2.3234, Estimated time left: 0:58:21\n","Step 33200/104948: Loss=2.6699, Estimated time left: 0:58:16\n","Step 33300/104948: Loss=2.7735, Estimated time left: 0:58:11\n","Step 33400/104948: Loss=2.6244, Estimated time left: 0:58:06\n","Step 33500/104948: Loss=1.3193, Estimated time left: 0:58:02\n","Step 33600/104948: Loss=1.4588, Estimated time left: 0:57:57\n","Step 33700/104948: Loss=2.9626, Estimated time left: 0:57:52\n","Step 33800/104948: Loss=1.6328, Estimated time left: 0:57:47\n","Step 33900/104948: Loss=3.6565, Estimated time left: 0:57:43\n","Step 34000/104948: Loss=2.3269, Estimated time left: 0:57:38\n","Step 34000: Loss=2.3269, LR=3.380103e-05, Accuracy=0.1887\n","Step 34100/104948: Loss=3.1123, Estimated time left: 0:57:33\n","Step 34200/104948: Loss=3.0750, Estimated time left: 0:57:28\n","Step 34300/104948: Loss=3.2984, Estimated time left: 0:57:23\n","Step 34400/104948: Loss=1.9290, Estimated time left: 0:57:19\n","Step 34500/104948: Loss=2.7089, Estimated time left: 0:57:14\n","Step 34600/104948: Loss=1.1701, Estimated time left: 0:57:09\n","Step 34700/104948: Loss=1.8317, Estimated time left: 0:57:04\n","Step 34800/104948: Loss=4.1786, Estimated time left: 0:56:59\n","Step 34900/104948: Loss=2.5936, Estimated time left: 0:56:54\n","Step 35000/104948: Loss=1.9950, Estimated time left: 0:56:49\n","Step 35000: Loss=1.9950, LR=3.332460e-05, Accuracy=0.1878\n","Step 35100/104948: Loss=2.8667, Estimated time left: 0:56:44\n","Step 35200/104948: Loss=3.0102, Estimated time left: 0:56:40\n","Step 35300/104948: Loss=3.1217, Estimated time left: 0:56:35\n","Step 35400/104948: Loss=3.5057, Estimated time left: 0:56:30\n","Step 35500/104948: Loss=3.0173, Estimated time left: 0:56:25\n","Step 35600/104948: Loss=1.3910, Estimated time left: 0:56:20\n","Step 35700/104948: Loss=2.0998, Estimated time left: 0:56:15\n","Step 35800/104948: Loss=1.8967, Estimated time left: 0:56:11\n","Step 35900/104948: Loss=2.6309, Estimated time left: 0:56:06\n","Step 36000/104948: Loss=2.9417, Estimated time left: 0:56:01\n","Step 36000: Loss=2.9417, LR=3.284817e-05, Accuracy=0.2093\n","Step 36100/104948: Loss=2.8394, Estimated time left: 0:55:57\n","Step 36200/104948: Loss=1.2643, Estimated time left: 0:55:52\n","Step 36300/104948: Loss=1.1203, Estimated time left: 0:55:47\n","Step 36400/104948: Loss=2.6575, Estimated time left: 0:55:42\n","Step 36500/104948: Loss=2.3595, Estimated time left: 0:55:37\n","Step 36600/104948: Loss=2.1029, Estimated time left: 0:55:32\n","Step 36700/104948: Loss=2.3580, Estimated time left: 0:55:28\n","Step 36800/104948: Loss=2.8117, Estimated time left: 0:55:23\n","Step 36900/104948: Loss=2.1724, Estimated time left: 0:55:18\n","Step 37000/104948: Loss=2.6460, Estimated time left: 0:55:13\n","Step 37000: Loss=2.6460, LR=3.237175e-05, Accuracy=0.1993\n","Step 37100/104948: Loss=2.4003, Estimated time left: 0:55:08\n","Step 37200/104948: Loss=1.8158, Estimated time left: 0:55:03\n","Step 37300/104948: Loss=3.0105, Estimated time left: 0:54:58\n","Step 37400/104948: Loss=1.7760, Estimated time left: 0:54:53\n","Step 37500/104948: Loss=2.0984, Estimated time left: 0:54:48\n","Step 37600/104948: Loss=2.7514, Estimated time left: 0:54:43\n","Step 37700/104948: Loss=2.1860, Estimated time left: 0:54:39\n","Step 37800/104948: Loss=2.0171, Estimated time left: 0:54:34\n","Step 37900/104948: Loss=1.9720, Estimated time left: 0:54:29\n","Step 38000/104948: Loss=2.9656, Estimated time left: 0:54:24\n","Step 38000: Loss=2.9656, LR=3.189532e-05, Accuracy=0.1942\n","Step 38100/104948: Loss=1.5809, Estimated time left: 0:54:19\n","Step 38200/104948: Loss=2.0743, Estimated time left: 0:54:15\n","Step 38300/104948: Loss=2.4833, Estimated time left: 0:54:10\n","Step 38400/104948: Loss=1.3731, Estimated time left: 0:54:05\n","Step 38500/104948: Loss=3.3781, Estimated time left: 0:54:00\n","Step 38600/104948: Loss=2.7336, Estimated time left: 0:53:56\n","Step 38700/104948: Loss=2.3339, Estimated time left: 0:53:51\n","Step 38800/104948: Loss=3.0228, Estimated time left: 0:53:46\n","Step 38900/104948: Loss=4.0208, Estimated time left: 0:53:41\n","Step 39000/104948: Loss=1.8041, Estimated time left: 0:53:36\n","Step 39000: Loss=1.8041, LR=3.141889e-05, Accuracy=0.2167\n","Step 39100/104948: Loss=2.8139, Estimated time left: 0:53:31\n","Step 39200/104948: Loss=3.4010, Estimated time left: 0:53:27\n","Step 39300/104948: Loss=3.2773, Estimated time left: 0:53:22\n","Step 39400/104948: Loss=3.8030, Estimated time left: 0:53:17\n","Step 39500/104948: Loss=2.1968, Estimated time left: 0:53:12\n","Step 39600/104948: Loss=2.2885, Estimated time left: 0:53:08\n","Step 39700/104948: Loss=4.4962, Estimated time left: 0:53:03\n","Step 39800/104948: Loss=3.7622, Estimated time left: 0:52:58\n","Step 39900/104948: Loss=2.7596, Estimated time left: 0:52:53\n","Step 40000/104948: Loss=2.2069, Estimated time left: 0:52:48\n","Step 40000: Loss=2.2069, LR=3.094247e-05, Accuracy=0.1986\n","Step 40100/104948: Loss=2.4162, Estimated time left: 0:52:43\n","Step 40200/104948: Loss=1.3624, Estimated time left: 0:52:38\n","Step 40300/104948: Loss=1.7922, Estimated time left: 0:52:34\n","Step 40400/104948: Loss=2.4569, Estimated time left: 0:52:29\n","Step 40500/104948: Loss=2.6809, Estimated time left: 0:52:24\n","Step 40600/104948: Loss=1.4009, Estimated time left: 0:52:19\n","Step 40700/104948: Loss=2.1276, Estimated time left: 0:52:14\n","Step 40800/104948: Loss=2.2994, Estimated time left: 0:52:09\n","Step 40900/104948: Loss=2.0046, Estimated time left: 0:52:04\n","Step 41000/104948: Loss=2.6969, Estimated time left: 0:52:00\n","Step 41000: Loss=2.6969, LR=3.046604e-05, Accuracy=0.2050\n","Step 41100/104948: Loss=2.9925, Estimated time left: 0:51:55\n","Step 41200/104948: Loss=3.0146, Estimated time left: 0:51:50\n","Step 41300/104948: Loss=3.4404, Estimated time left: 0:51:45\n","Step 41400/104948: Loss=1.7086, Estimated time left: 0:51:41\n","Step 41500/104948: Loss=2.0063, Estimated time left: 0:51:36\n","Step 41600/104948: Loss=3.7265, Estimated time left: 0:51:31\n","Step 41700/104948: Loss=2.2371, Estimated time left: 0:51:26\n","Step 41800/104948: Loss=1.3712, Estimated time left: 0:51:21\n","Step 41900/104948: Loss=3.5408, Estimated time left: 0:51:17\n","Step 42000/104948: Loss=1.2134, Estimated time left: 0:51:12\n","Step 42000: Loss=1.2134, LR=2.998961e-05, Accuracy=0.2011\n","Step 42100/104948: Loss=2.8773, Estimated time left: 0:51:07\n","Step 42200/104948: Loss=2.2265, Estimated time left: 0:51:03\n","Step 42300/104948: Loss=1.3703, Estimated time left: 0:50:58\n","Step 42400/104948: Loss=2.4486, Estimated time left: 0:50:53\n","Step 42500/104948: Loss=3.0474, Estimated time left: 0:50:48\n","Step 42600/104948: Loss=2.4202, Estimated time left: 0:50:43\n","Step 42700/104948: Loss=2.4543, Estimated time left: 0:50:39\n","Step 42800/104948: Loss=2.7656, Estimated time left: 0:50:34\n","Step 42900/104948: Loss=2.8473, Estimated time left: 0:50:29\n","Step 43000/104948: Loss=2.4402, Estimated time left: 0:50:24\n","Step 43000: Loss=2.4402, LR=2.951319e-05, Accuracy=0.1750\n","Step 43100/104948: Loss=2.0845, Estimated time left: 0:50:19\n","Step 43200/104948: Loss=2.8973, Estimated time left: 0:50:14\n","Step 43300/104948: Loss=2.9103, Estimated time left: 0:50:10\n","Step 43400/104948: Loss=1.7289, Estimated time left: 0:50:05\n","Step 43500/104948: Loss=1.0354, Estimated time left: 0:50:00\n","Step 43600/104948: Loss=2.5768, Estimated time left: 0:49:55\n","Step 43700/104948: Loss=1.5457, Estimated time left: 0:49:50\n","Step 43800/104948: Loss=1.9344, Estimated time left: 0:49:45\n","Step 43900/104948: Loss=1.8025, Estimated time left: 0:49:40\n","Step 44000/104948: Loss=1.6457, Estimated time left: 0:49:36\n","Step 44000: Loss=1.6457, LR=2.903676e-05, Accuracy=0.2015\n","Step 44100/104948: Loss=1.6919, Estimated time left: 0:49:31\n","Step 44200/104948: Loss=2.6683, Estimated time left: 0:49:26\n","Step 44300/104948: Loss=1.4945, Estimated time left: 0:49:21\n","Step 44400/104948: Loss=1.8785, Estimated time left: 0:49:16\n","Step 44500/104948: Loss=1.5819, Estimated time left: 0:49:12\n","Step 44600/104948: Loss=3.0423, Estimated time left: 0:49:07\n","Step 44700/104948: Loss=2.3325, Estimated time left: 0:49:02\n","Step 44800/104948: Loss=2.2690, Estimated time left: 0:48:57\n","Step 44900/104948: Loss=2.7888, Estimated time left: 0:48:52\n","Step 45000/104948: Loss=2.4022, Estimated time left: 0:48:48\n","Step 45000: Loss=2.4022, LR=2.856033e-05, Accuracy=0.2162\n","Step 45100/104948: Loss=1.9183, Estimated time left: 0:48:43\n","Step 45200/104948: Loss=1.6806, Estimated time left: 0:48:38\n","Step 45300/104948: Loss=1.7839, Estimated time left: 0:48:33\n","Step 45400/104948: Loss=1.4213, Estimated time left: 0:48:28\n","Step 45500/104948: Loss=1.2299, Estimated time left: 0:48:23\n","Step 45600/104948: Loss=2.6087, Estimated time left: 0:48:19\n","Step 45700/104948: Loss=3.2340, Estimated time left: 0:48:14\n","Step 45800/104948: Loss=1.5689, Estimated time left: 0:48:09\n","Step 45900/104948: Loss=3.8683, Estimated time left: 0:48:04\n","Step 46000/104948: Loss=2.9000, Estimated time left: 0:47:59\n","Step 46000: Loss=2.9000, LR=2.808391e-05, Accuracy=0.1856\n","Step 46100/104948: Loss=2.7944, Estimated time left: 0:47:54\n","Step 46200/104948: Loss=2.2832, Estimated time left: 0:47:49\n","Step 46300/104948: Loss=2.1134, Estimated time left: 0:47:44\n","Step 46400/104948: Loss=3.5447, Estimated time left: 0:47:40\n","Step 46500/104948: Loss=2.9677, Estimated time left: 0:47:35\n","Step 46600/104948: Loss=3.9726, Estimated time left: 0:47:30\n","Step 46700/104948: Loss=2.4067, Estimated time left: 0:47:25\n","Step 46800/104948: Loss=1.1632, Estimated time left: 0:47:20\n","Step 46900/104948: Loss=3.0178, Estimated time left: 0:47:15\n","Step 47000/104948: Loss=2.8410, Estimated time left: 0:47:11\n","Step 47000: Loss=2.8410, LR=2.760748e-05, Accuracy=0.1932\n","Step 47100/104948: Loss=1.6583, Estimated time left: 0:47:06\n","Step 47200/104948: Loss=4.0019, Estimated time left: 0:47:01\n","Step 47300/104948: Loss=4.9275, Estimated time left: 0:46:57\n","Step 47400/104948: Loss=2.3866, Estimated time left: 0:46:52\n","Step 47500/104948: Loss=3.9414, Estimated time left: 0:46:47\n","Step 47600/104948: Loss=3.4112, Estimated time left: 0:46:42\n","Step 47700/104948: Loss=2.5134, Estimated time left: 0:46:37\n","Step 47800/104948: Loss=1.3131, Estimated time left: 0:46:33\n","Step 47900/104948: Loss=2.1681, Estimated time left: 0:46:28\n","Step 48000/104948: Loss=3.9354, Estimated time left: 0:46:23\n","Step 48000: Loss=3.9354, LR=2.713106e-05, Accuracy=0.2062\n","Step 48100/104948: Loss=1.7426, Estimated time left: 0:46:18\n","Step 48200/104948: Loss=2.9059, Estimated time left: 0:46:13\n","Step 48300/104948: Loss=1.6973, Estimated time left: 0:46:08\n","Step 48400/104948: Loss=3.0284, Estimated time left: 0:46:04\n","Step 48500/104948: Loss=3.1595, Estimated time left: 0:45:59\n","Step 48600/104948: Loss=3.4297, Estimated time left: 0:45:54\n","Step 48700/104948: Loss=3.7831, Estimated time left: 0:45:49\n","Step 48800/104948: Loss=4.5649, Estimated time left: 0:45:44\n","Step 48900/104948: Loss=3.0818, Estimated time left: 0:45:40\n","Step 49000/104948: Loss=1.9919, Estimated time left: 0:45:35\n","Step 49000: Loss=1.9919, LR=2.665463e-05, Accuracy=0.2232\n","Step 49100/104948: Loss=2.9265, Estimated time left: 0:45:31\n","Step 49200/104948: Loss=2.2163, Estimated time left: 0:45:26\n","Step 49300/104948: Loss=2.0744, Estimated time left: 0:45:21\n","Step 49400/104948: Loss=1.8503, Estimated time left: 0:45:16\n","Step 49500/104948: Loss=2.4294, Estimated time left: 0:45:11\n","Step 49600/104948: Loss=3.0304, Estimated time left: 0:45:06\n","Step 49700/104948: Loss=1.8235, Estimated time left: 0:45:01\n","Step 49800/104948: Loss=1.3850, Estimated time left: 0:44:57\n","Step 49900/104948: Loss=3.6513, Estimated time left: 0:44:52\n","Step 50000/104948: Loss=2.1534, Estimated time left: 0:44:47\n","Step 50000: Loss=2.1534, LR=2.617820e-05, Accuracy=0.2038\n","Step 50100/104948: Loss=1.9862, Estimated time left: 0:44:42\n","Step 50200/104948: Loss=2.7939, Estimated time left: 0:44:37\n","Step 50300/104948: Loss=3.8430, Estimated time left: 0:44:33\n","Step 50400/104948: Loss=3.0602, Estimated time left: 0:44:28\n","Step 50500/104948: Loss=2.7158, Estimated time left: 0:44:23\n","Step 50600/104948: Loss=3.0083, Estimated time left: 0:44:18\n","Step 50700/104948: Loss=4.1107, Estimated time left: 0:44:13\n","Step 50800/104948: Loss=2.8222, Estimated time left: 0:44:08\n","Step 50900/104948: Loss=3.2422, Estimated time left: 0:44:03\n","Step 51000/104948: Loss=1.5985, Estimated time left: 0:43:59\n","Step 51000: Loss=1.5985, LR=2.570178e-05, Accuracy=0.2034\n","Step 51100/104948: Loss=3.2674, Estimated time left: 0:43:54\n","Step 51200/104948: Loss=3.1976, Estimated time left: 0:43:49\n","Step 51300/104948: Loss=4.9906, Estimated time left: 0:43:44\n","Step 51400/104948: Loss=3.0975, Estimated time left: 0:43:39\n","Step 51500/104948: Loss=1.8956, Estimated time left: 0:43:34\n","Step 51600/104948: Loss=1.6511, Estimated time left: 0:43:30\n","Step 51700/104948: Loss=2.8520, Estimated time left: 0:43:25\n","Step 51800/104948: Loss=3.0992, Estimated time left: 0:43:20\n","Step 51900/104948: Loss=2.1501, Estimated time left: 0:43:15\n","Step 52000/104948: Loss=1.2591, Estimated time left: 0:43:10\n","Step 52000: Loss=1.2591, LR=2.522535e-05, Accuracy=0.2045\n","Step 52100/104948: Loss=2.8250, Estimated time left: 0:43:06\n","Step 52200/104948: Loss=2.3612, Estimated time left: 0:43:01\n","Step 52300/104948: Loss=3.2203, Estimated time left: 0:42:56\n","Step 52400/104948: Loss=1.7169, Estimated time left: 0:42:51\n","Step 52500/104948: Loss=3.2189, Estimated time left: 0:42:46\n","Step 52600/104948: Loss=1.6125, Estimated time left: 0:42:41\n","Step 52700/104948: Loss=2.9096, Estimated time left: 0:42:36\n","Step 52800/104948: Loss=3.8955, Estimated time left: 0:42:32\n","Step 52900/104948: Loss=1.1123, Estimated time left: 0:42:27\n","Step 53000/104948: Loss=2.4946, Estimated time left: 0:42:22\n","Step 53000: Loss=2.4946, LR=2.474892e-05, Accuracy=0.2062\n","Step 53100/104948: Loss=2.7307, Estimated time left: 0:42:17\n","Step 53200/104948: Loss=2.1547, Estimated time left: 0:42:12\n","Step 53300/104948: Loss=2.2888, Estimated time left: 0:42:08\n","Step 53400/104948: Loss=1.8776, Estimated time left: 0:42:03\n","Step 53500/104948: Loss=2.3973, Estimated time left: 0:41:58\n","Step 53600/104948: Loss=2.1214, Estimated time left: 0:41:53\n","Step 53700/104948: Loss=2.9949, Estimated time left: 0:41:48\n","Step 53800/104948: Loss=2.6647, Estimated time left: 0:41:43\n","Step 53900/104948: Loss=2.1039, Estimated time left: 0:41:39\n","Step 54000/104948: Loss=1.4329, Estimated time left: 0:41:34\n","Step 54000: Loss=1.4329, LR=2.427250e-05, Accuracy=0.1621\n","Step 54100/104948: Loss=1.8742, Estimated time left: 0:41:29\n","Step 54200/104948: Loss=2.0700, Estimated time left: 0:41:24\n","Step 54300/104948: Loss=2.4047, Estimated time left: 0:41:19\n","Step 54400/104948: Loss=2.7121, Estimated time left: 0:41:15\n","Step 54500/104948: Loss=3.1114, Estimated time left: 0:41:10\n","Step 54600/104948: Loss=2.9758, Estimated time left: 0:41:05\n","Step 54700/104948: Loss=2.2810, Estimated time left: 0:41:00\n","Step 54800/104948: Loss=2.3900, Estimated time left: 0:40:55\n","Step 54900/104948: Loss=3.4407, Estimated time left: 0:40:50\n","Step 55000/104948: Loss=2.1463, Estimated time left: 0:40:45\n","Step 55000: Loss=2.1463, LR=2.379607e-05, Accuracy=0.2093\n","Step 55100/104948: Loss=3.6009, Estimated time left: 0:40:40\n","Step 55200/104948: Loss=2.5492, Estimated time left: 0:40:36\n","Step 55300/104948: Loss=3.8003, Estimated time left: 0:40:31\n","Step 55400/104948: Loss=3.1779, Estimated time left: 0:40:26\n","Step 55500/104948: Loss=2.6326, Estimated time left: 0:40:21\n","Step 55600/104948: Loss=2.5592, Estimated time left: 0:40:16\n","Step 55700/104948: Loss=3.1015, Estimated time left: 0:40:11\n","Step 55800/104948: Loss=2.1852, Estimated time left: 0:40:06\n","Step 55900/104948: Loss=1.4739, Estimated time left: 0:40:01\n","Step 56000/104948: Loss=2.2437, Estimated time left: 0:39:56\n","Step 56000: Loss=2.2437, LR=2.331964e-05, Accuracy=0.2173\n","Step 56100/104948: Loss=2.8720, Estimated time left: 0:39:52\n","Step 56200/104948: Loss=3.1823, Estimated time left: 0:39:47\n","Step 56300/104948: Loss=3.1472, Estimated time left: 0:39:42\n","Step 56400/104948: Loss=2.7027, Estimated time left: 0:39:37\n","Step 56500/104948: Loss=2.6167, Estimated time left: 0:39:32\n","Step 56600/104948: Loss=3.4184, Estimated time left: 0:39:28\n","Step 56700/104948: Loss=1.7971, Estimated time left: 0:39:23\n","Step 56800/104948: Loss=3.5566, Estimated time left: 0:39:18\n","Step 56900/104948: Loss=3.8990, Estimated time left: 0:39:13\n","Step 57000/104948: Loss=2.0374, Estimated time left: 0:39:08\n","Step 57000: Loss=2.0374, LR=2.284322e-05, Accuracy=0.2070\n","Step 57100/104948: Loss=2.5329, Estimated time left: 0:39:03\n","Step 57200/104948: Loss=1.2802, Estimated time left: 0:38:58\n","Step 57300/104948: Loss=2.9162, Estimated time left: 0:38:54\n","Step 57400/104948: Loss=2.6038, Estimated time left: 0:38:49\n","Step 57500/104948: Loss=2.5115, Estimated time left: 0:38:44\n","Step 57600/104948: Loss=2.4060, Estimated time left: 0:38:39\n","Step 57700/104948: Loss=3.9405, Estimated time left: 0:38:34\n","Step 57800/104948: Loss=2.7501, Estimated time left: 0:38:29\n","Step 57900/104948: Loss=1.7058, Estimated time left: 0:38:24\n","Step 58000/104948: Loss=4.3551, Estimated time left: 0:38:19\n","Step 58000: Loss=4.3551, LR=2.236679e-05, Accuracy=0.1820\n","Step 58100/104948: Loss=4.8412, Estimated time left: 0:38:14\n","Step 58200/104948: Loss=2.5090, Estimated time left: 0:38:09\n","Step 58300/104948: Loss=2.4056, Estimated time left: 0:38:05\n","Step 58400/104948: Loss=1.7669, Estimated time left: 0:38:00\n","Step 58500/104948: Loss=3.1926, Estimated time left: 0:37:55\n","Step 58600/104948: Loss=2.8499, Estimated time left: 0:37:50\n","Step 58700/104948: Loss=1.2903, Estimated time left: 0:37:45\n","Step 58800/104948: Loss=2.5932, Estimated time left: 0:37:40\n","Step 58900/104948: Loss=2.3094, Estimated time left: 0:37:35\n","Step 59000/104948: Loss=1.7754, Estimated time left: 0:37:30\n","Step 59000: Loss=1.7754, LR=2.189036e-05, Accuracy=0.1885\n","Step 59100/104948: Loss=1.3061, Estimated time left: 0:37:25\n","Step 59200/104948: Loss=3.7411, Estimated time left: 0:37:20\n","Step 59300/104948: Loss=2.1367, Estimated time left: 0:37:15\n","Step 59400/104948: Loss=1.7418, Estimated time left: 0:37:11\n","Step 59500/104948: Loss=2.7689, Estimated time left: 0:37:06\n","Step 59600/104948: Loss=3.0794, Estimated time left: 0:37:01\n","Step 59700/104948: Loss=2.7355, Estimated time left: 0:36:56\n","Step 59800/104948: Loss=1.5328, Estimated time left: 0:36:51\n","Step 59900/104948: Loss=2.9935, Estimated time left: 0:36:46\n","Step 60000/104948: Loss=3.0799, Estimated time left: 0:36:41\n","Step 60000: Loss=3.0799, LR=2.141394e-05, Accuracy=0.1946\n","Step 60100/104948: Loss=2.4056, Estimated time left: 0:36:36\n","Step 60200/104948: Loss=3.2464, Estimated time left: 0:36:31\n","Step 60300/104948: Loss=3.0665, Estimated time left: 0:36:26\n","Step 60400/104948: Loss=4.4854, Estimated time left: 0:36:21\n","Step 60500/104948: Loss=1.9786, Estimated time left: 0:36:16\n","Step 60600/104948: Loss=5.2040, Estimated time left: 0:36:12\n","Step 60700/104948: Loss=2.6660, Estimated time left: 0:36:07\n","Step 60800/104948: Loss=2.5068, Estimated time left: 0:36:02\n","Step 60900/104948: Loss=3.0048, Estimated time left: 0:35:57\n","Step 61000/104948: Loss=1.4491, Estimated time left: 0:35:52\n","Step 61000: Loss=1.4491, LR=2.093751e-05, Accuracy=0.1806\n","Step 61100/104948: Loss=3.2154, Estimated time left: 0:35:47\n","Step 61200/104948: Loss=1.0752, Estimated time left: 0:35:42\n","Step 61300/104948: Loss=1.6227, Estimated time left: 0:35:37\n","Step 61400/104948: Loss=1.6005, Estimated time left: 0:35:33\n","Step 61500/104948: Loss=2.2816, Estimated time left: 0:35:28\n","Step 61600/104948: Loss=2.9833, Estimated time left: 0:35:23\n","Step 61700/104948: Loss=2.3083, Estimated time left: 0:35:18\n","Step 61800/104948: Loss=1.9384, Estimated time left: 0:35:13\n","Step 61900/104948: Loss=2.5034, Estimated time left: 0:35:08\n","Step 62000/104948: Loss=1.5875, Estimated time left: 0:35:03\n","Step 62000: Loss=1.5875, LR=2.046109e-05, Accuracy=0.1989\n","Step 62100/104948: Loss=2.3998, Estimated time left: 0:34:58\n","Step 62200/104948: Loss=3.1491, Estimated time left: 0:34:53\n","Step 62300/104948: Loss=2.3919, Estimated time left: 0:34:49\n","Step 62400/104948: Loss=4.1428, Estimated time left: 0:34:44\n","Step 62500/104948: Loss=2.8284, Estimated time left: 0:34:39\n","Step 62600/104948: Loss=2.5194, Estimated time left: 0:34:34\n","Step 62700/104948: Loss=2.9647, Estimated time left: 0:34:29\n","Step 62800/104948: Loss=2.9595, Estimated time left: 0:34:24\n","Step 62900/104948: Loss=2.3026, Estimated time left: 0:34:19\n","Step 63000/104948: Loss=1.4715, Estimated time left: 0:34:14\n","Step 63000: Loss=1.4715, LR=1.998466e-05, Accuracy=0.2170\n","Step 63100/104948: Loss=2.2406, Estimated time left: 0:34:09\n","Step 63200/104948: Loss=3.6878, Estimated time left: 0:34:04\n","Step 63300/104948: Loss=1.5936, Estimated time left: 0:34:00\n","Step 63400/104948: Loss=1.8338, Estimated time left: 0:33:55\n","Step 63500/104948: Loss=2.7458, Estimated time left: 0:33:50\n","Step 63600/104948: Loss=2.9966, Estimated time left: 0:33:45\n","Step 63700/104948: Loss=1.7446, Estimated time left: 0:33:40\n","Step 63800/104948: Loss=3.8045, Estimated time left: 0:33:35\n","Step 63900/104948: Loss=3.8961, Estimated time left: 0:33:30\n","Step 64000/104948: Loss=2.3610, Estimated time left: 0:33:25\n","Step 64000: Loss=2.3610, LR=1.950823e-05, Accuracy=0.1830\n","Step 64100/104948: Loss=2.9939, Estimated time left: 0:33:21\n","Step 64200/104948: Loss=2.5845, Estimated time left: 0:33:16\n","Step 64300/104948: Loss=2.0795, Estimated time left: 0:33:11\n","Step 64400/104948: Loss=2.3231, Estimated time left: 0:33:06\n","Step 64500/104948: Loss=3.7430, Estimated time left: 0:33:01\n","Step 64600/104948: Loss=1.7237, Estimated time left: 0:32:56\n","Step 64700/104948: Loss=1.5282, Estimated time left: 0:32:51\n","Step 64800/104948: Loss=1.9107, Estimated time left: 0:32:46\n","Step 64900/104948: Loss=2.9133, Estimated time left: 0:32:41\n","Step 65000/104948: Loss=1.5428, Estimated time left: 0:32:37\n","Step 65000: Loss=1.5428, LR=1.903181e-05, Accuracy=0.1975\n","Step 65100/104948: Loss=2.6524, Estimated time left: 0:32:32\n","Step 65200/104948: Loss=1.8246, Estimated time left: 0:32:27\n","Step 65300/104948: Loss=2.5484, Estimated time left: 0:32:22\n","Step 65400/104948: Loss=3.2527, Estimated time left: 0:32:17\n","Step 65500/104948: Loss=2.8904, Estimated time left: 0:32:12\n","Step 65600/104948: Loss=2.9307, Estimated time left: 0:32:07\n","Step 65700/104948: Loss=1.3529, Estimated time left: 0:32:02\n","Step 65800/104948: Loss=3.1192, Estimated time left: 0:31:58\n","Step 65900/104948: Loss=4.1296, Estimated time left: 0:31:53\n","Step 66000/104948: Loss=3.2784, Estimated time left: 0:31:48\n","Step 66000: Loss=3.2784, LR=1.855538e-05, Accuracy=0.2068\n","Step 66100/104948: Loss=2.3714, Estimated time left: 0:31:43\n","Step 66200/104948: Loss=2.1288, Estimated time left: 0:31:38\n","Step 66300/104948: Loss=3.4797, Estimated time left: 0:31:33\n","Step 66400/104948: Loss=2.7917, Estimated time left: 0:31:28\n","Step 66500/104948: Loss=2.8735, Estimated time left: 0:31:23\n","Step 66600/104948: Loss=3.8346, Estimated time left: 0:31:19\n","Step 66700/104948: Loss=3.1505, Estimated time left: 0:31:14\n","Step 66800/104948: Loss=4.0897, Estimated time left: 0:31:09\n","Step 66900/104948: Loss=2.0157, Estimated time left: 0:31:04\n","Step 67000/104948: Loss=2.7514, Estimated time left: 0:30:59\n","Step 67000: Loss=2.7514, LR=1.807895e-05, Accuracy=0.2034\n","Step 67100/104948: Loss=3.2464, Estimated time left: 0:30:54\n","Step 67200/104948: Loss=2.9681, Estimated time left: 0:30:49\n","Step 67300/104948: Loss=2.7497, Estimated time left: 0:30:44\n","Step 67400/104948: Loss=1.8742, Estimated time left: 0:30:40\n","Step 67500/104948: Loss=4.0525, Estimated time left: 0:30:35\n","Step 67600/104948: Loss=3.8558, Estimated time left: 0:30:30\n","Step 67700/104948: Loss=2.1507, Estimated time left: 0:30:25\n","Step 67800/104948: Loss=1.4036, Estimated time left: 0:30:20\n","Step 67900/104948: Loss=3.3282, Estimated time left: 0:30:15\n","Step 68000/104948: Loss=3.1729, Estimated time left: 0:30:10\n","Step 68000: Loss=3.1729, LR=1.760253e-05, Accuracy=0.1822\n","Step 68100/104948: Loss=3.3528, Estimated time left: 0:30:05\n","Step 68200/104948: Loss=1.3898, Estimated time left: 0:30:00\n","Step 68300/104948: Loss=2.8355, Estimated time left: 0:29:56\n","Step 68400/104948: Loss=1.7509, Estimated time left: 0:29:51\n","Step 68500/104948: Loss=3.8371, Estimated time left: 0:29:46\n","Step 68600/104948: Loss=2.7785, Estimated time left: 0:29:41\n","Step 68700/104948: Loss=1.8628, Estimated time left: 0:29:36\n","Step 68800/104948: Loss=3.0388, Estimated time left: 0:29:31\n","Step 68900/104948: Loss=1.5673, Estimated time left: 0:29:26\n","Step 69000/104948: Loss=1.7047, Estimated time left: 0:29:21\n","Step 69000: Loss=1.7047, LR=1.712610e-05, Accuracy=0.1825\n","Step 69100/104948: Loss=2.2171, Estimated time left: 0:29:17\n","Step 69200/104948: Loss=3.4084, Estimated time left: 0:29:12\n","Step 69300/104948: Loss=1.1492, Estimated time left: 0:29:07\n","Step 69400/104948: Loss=3.5129, Estimated time left: 0:29:02\n","Step 69500/104948: Loss=1.5849, Estimated time left: 0:28:57\n","Step 69600/104948: Loss=2.8390, Estimated time left: 0:28:52\n","Step 69700/104948: Loss=4.1062, Estimated time left: 0:28:47\n","Step 69800/104948: Loss=2.4510, Estimated time left: 0:28:42\n","Step 69900/104948: Loss=1.7134, Estimated time left: 0:28:37\n","Step 70000/104948: Loss=1.6566, Estimated time left: 0:28:33\n","Step 70000: Loss=1.6566, LR=1.664967e-05, Accuracy=0.2040\n","Step 70100/104948: Loss=2.6586, Estimated time left: 0:28:28\n","Step 70200/104948: Loss=2.6847, Estimated time left: 0:28:23\n","Step 70300/104948: Loss=2.7566, Estimated time left: 0:28:18\n","Step 70400/104948: Loss=1.2138, Estimated time left: 0:28:13\n","Step 70500/104948: Loss=2.9756, Estimated time left: 0:28:08\n","Step 70600/104948: Loss=2.4483, Estimated time left: 0:28:03\n","Step 70700/104948: Loss=1.3093, Estimated time left: 0:27:58\n","Step 70800/104948: Loss=3.3038, Estimated time left: 0:27:54\n","Step 70900/104948: Loss=2.3387, Estimated time left: 0:27:49\n","Step 71000/104948: Loss=2.4427, Estimated time left: 0:27:44\n","Step 71000: Loss=2.4427, LR=1.617325e-05, Accuracy=0.1975\n","Step 71100/104948: Loss=1.3230, Estimated time left: 0:27:39\n","Step 71200/104948: Loss=2.8225, Estimated time left: 0:27:34\n","Step 71300/104948: Loss=2.2121, Estimated time left: 0:27:29\n","Step 71400/104948: Loss=2.1947, Estimated time left: 0:27:24\n","Step 71500/104948: Loss=3.7141, Estimated time left: 0:27:19\n","Step 71600/104948: Loss=1.5850, Estimated time left: 0:27:14\n","Step 71700/104948: Loss=3.0403, Estimated time left: 0:27:09\n","Step 71800/104948: Loss=3.0363, Estimated time left: 0:27:05\n","Step 71900/104948: Loss=3.9119, Estimated time left: 0:27:00\n","Step 72000/104948: Loss=4.6645, Estimated time left: 0:26:55\n","Step 72000: Loss=4.6645, LR=1.569682e-05, Accuracy=0.1974\n","Step 72100/104948: Loss=4.1147, Estimated time left: 0:26:50\n","Step 72200/104948: Loss=2.4279, Estimated time left: 0:26:45\n","Step 72300/104948: Loss=2.1707, Estimated time left: 0:26:40\n","Step 72400/104948: Loss=2.1951, Estimated time left: 0:26:35\n","Step 72500/104948: Loss=2.6007, Estimated time left: 0:26:30\n","Step 72600/104948: Loss=4.2165, Estimated time left: 0:26:25\n","Step 72700/104948: Loss=3.0386, Estimated time left: 0:26:21\n","Step 72800/104948: Loss=2.1745, Estimated time left: 0:26:16\n","Step 72900/104948: Loss=1.8740, Estimated time left: 0:26:11\n","Step 73000/104948: Loss=3.1120, Estimated time left: 0:26:06\n","Step 73000: Loss=3.1120, LR=1.522039e-05, Accuracy=0.2007\n","Step 73100/104948: Loss=3.4126, Estimated time left: 0:26:01\n","Step 73200/104948: Loss=3.5163, Estimated time left: 0:25:56\n","Step 73300/104948: Loss=3.4498, Estimated time left: 0:25:51\n","Step 73400/104948: Loss=2.5196, Estimated time left: 0:25:47\n","Step 73500/104948: Loss=2.7260, Estimated time left: 0:25:42\n","Step 73600/104948: Loss=1.8696, Estimated time left: 0:25:37\n","Step 73700/104948: Loss=2.0149, Estimated time left: 0:25:32\n","Step 73800/104948: Loss=2.3431, Estimated time left: 0:25:27\n","Step 73900/104948: Loss=2.9976, Estimated time left: 0:25:22\n","Step 74000/104948: Loss=2.9330, Estimated time left: 0:25:17\n","Step 74000: Loss=2.9330, LR=1.474397e-05, Accuracy=0.1832\n","Step 74100/104948: Loss=1.8720, Estimated time left: 0:25:12\n","Step 74200/104948: Loss=3.0469, Estimated time left: 0:25:07\n","Step 74300/104948: Loss=2.5272, Estimated time left: 0:25:03\n","Step 74400/104948: Loss=2.4659, Estimated time left: 0:24:58\n","Step 74500/104948: Loss=3.0309, Estimated time left: 0:24:53\n","Step 74600/104948: Loss=2.3191, Estimated time left: 0:24:48\n","Step 74700/104948: Loss=2.8514, Estimated time left: 0:24:43\n","Step 74800/104948: Loss=2.4583, Estimated time left: 0:24:38\n","Step 74900/104948: Loss=2.3041, Estimated time left: 0:24:33\n","Step 75000/104948: Loss=1.2104, Estimated time left: 0:24:28\n","Step 75000: Loss=1.2104, LR=1.426754e-05, Accuracy=0.1896\n","Step 75100/104948: Loss=3.3156, Estimated time left: 0:24:23\n","Step 75200/104948: Loss=1.4878, Estimated time left: 0:24:19\n","Step 75300/104948: Loss=1.5060, Estimated time left: 0:24:14\n","Step 75400/104948: Loss=2.7849, Estimated time left: 0:24:09\n","Step 75500/104948: Loss=1.6831, Estimated time left: 0:24:04\n","Step 75600/104948: Loss=3.4633, Estimated time left: 0:23:59\n","Step 75700/104948: Loss=3.5160, Estimated time left: 0:23:54\n","Step 75800/104948: Loss=2.7396, Estimated time left: 0:23:49\n","Step 75900/104948: Loss=2.9365, Estimated time left: 0:23:44\n","Step 76000/104948: Loss=2.2360, Estimated time left: 0:23:39\n","Step 76000: Loss=2.2360, LR=1.379112e-05, Accuracy=0.2061\n","Step 76100/104948: Loss=2.6028, Estimated time left: 0:23:34\n","Step 76200/104948: Loss=2.1816, Estimated time left: 0:23:30\n","Step 76300/104948: Loss=1.7815, Estimated time left: 0:23:25\n","Step 76400/104948: Loss=2.1048, Estimated time left: 0:23:20\n","Step 76500/104948: Loss=2.4317, Estimated time left: 0:23:15\n","Step 76600/104948: Loss=1.5874, Estimated time left: 0:23:10\n","Step 76700/104948: Loss=1.6625, Estimated time left: 0:23:05\n","Step 76800/104948: Loss=2.6368, Estimated time left: 0:23:00\n","Step 76900/104948: Loss=2.5230, Estimated time left: 0:22:55\n","Step 77000/104948: Loss=2.2071, Estimated time left: 0:22:50\n","Step 77000: Loss=2.2071, LR=1.331469e-05, Accuracy=0.2034\n","Step 77100/104948: Loss=1.6195, Estimated time left: 0:22:46\n","Step 77200/104948: Loss=2.0507, Estimated time left: 0:22:41\n","Step 77300/104948: Loss=2.9390, Estimated time left: 0:22:36\n","Step 77400/104948: Loss=1.4152, Estimated time left: 0:22:31\n","Step 77500/104948: Loss=2.3636, Estimated time left: 0:22:26\n","Step 77600/104948: Loss=3.1568, Estimated time left: 0:22:21\n","Step 77700/104948: Loss=2.7735, Estimated time left: 0:22:16\n","Step 77800/104948: Loss=2.2939, Estimated time left: 0:22:11\n","Step 77900/104948: Loss=3.9460, Estimated time left: 0:22:07\n","Step 78000/104948: Loss=2.3300, Estimated time left: 0:22:02\n","Step 78000: Loss=2.3300, LR=1.283826e-05, Accuracy=0.1659\n","Step 78100/104948: Loss=1.9508, Estimated time left: 0:21:57\n","Step 78200/104948: Loss=2.7090, Estimated time left: 0:21:52\n","Step 78300/104948: Loss=2.3207, Estimated time left: 0:21:47\n","Step 78400/104948: Loss=1.8393, Estimated time left: 0:21:42\n","Step 78500/104948: Loss=3.1750, Estimated time left: 0:21:37\n","Step 78600/104948: Loss=2.2736, Estimated time left: 0:21:32\n","Step 78700/104948: Loss=1.2370, Estimated time left: 0:21:27\n","Step 78800/104948: Loss=1.5132, Estimated time left: 0:21:23\n","Step 78900/104948: Loss=3.7935, Estimated time left: 0:21:18\n","Step 79000/104948: Loss=3.7169, Estimated time left: 0:21:13\n","Step 79000: Loss=3.7169, LR=1.236184e-05, Accuracy=0.2038\n","Step 79100/104948: Loss=1.8787, Estimated time left: 0:21:08\n","Step 79200/104948: Loss=3.2834, Estimated time left: 0:21:03\n","Step 79300/104948: Loss=3.3721, Estimated time left: 0:20:58\n","Step 79400/104948: Loss=2.6849, Estimated time left: 0:20:53\n","Step 79500/104948: Loss=2.3504, Estimated time left: 0:20:48\n","Step 79600/104948: Loss=2.3297, Estimated time left: 0:20:44\n","Step 79700/104948: Loss=2.7136, Estimated time left: 0:20:39\n","Step 79800/104948: Loss=2.0872, Estimated time left: 0:20:34\n","Step 79900/104948: Loss=2.8554, Estimated time left: 0:20:29\n","Step 80000/104948: Loss=2.3562, Estimated time left: 0:20:24\n","Step 80000: Loss=2.3562, LR=1.188541e-05, Accuracy=0.2073\n","Step 80100/104948: Loss=1.9098, Estimated time left: 0:20:19\n","Step 80200/104948: Loss=2.4285, Estimated time left: 0:20:14\n","Step 80300/104948: Loss=3.1198, Estimated time left: 0:20:09\n","Step 80400/104948: Loss=2.8442, Estimated time left: 0:20:04\n","Step 80500/104948: Loss=2.0743, Estimated time left: 0:19:59\n","Step 80600/104948: Loss=2.5406, Estimated time left: 0:19:55\n","Step 80700/104948: Loss=1.3962, Estimated time left: 0:19:50\n","Step 80800/104948: Loss=4.0433, Estimated time left: 0:19:45\n","Step 80900/104948: Loss=1.8991, Estimated time left: 0:19:40\n","Step 81000/104948: Loss=2.4067, Estimated time left: 0:19:35\n","Step 81000: Loss=2.4067, LR=1.140898e-05, Accuracy=0.2334\n","Step 81100/104948: Loss=2.7653, Estimated time left: 0:19:30\n","Step 81200/104948: Loss=2.0249, Estimated time left: 0:19:25\n","Step 81300/104948: Loss=2.0118, Estimated time left: 0:19:20\n","Step 81400/104948: Loss=2.5197, Estimated time left: 0:19:15\n","Step 81500/104948: Loss=2.5479, Estimated time left: 0:19:11\n","Step 81600/104948: Loss=2.4794, Estimated time left: 0:19:06\n","Step 81700/104948: Loss=2.6368, Estimated time left: 0:19:01\n","Step 81800/104948: Loss=2.2952, Estimated time left: 0:18:56\n","Step 81900/104948: Loss=1.9881, Estimated time left: 0:18:51\n","Step 82000/104948: Loss=3.8432, Estimated time left: 0:18:46\n","Step 82000: Loss=3.8432, LR=1.093256e-05, Accuracy=0.2119\n","Step 82100/104948: Loss=1.5940, Estimated time left: 0:18:41\n","Step 82200/104948: Loss=3.3942, Estimated time left: 0:18:36\n","Step 82300/104948: Loss=2.4750, Estimated time left: 0:18:31\n","Step 82400/104948: Loss=2.7706, Estimated time left: 0:18:26\n","Step 82500/104948: Loss=3.5016, Estimated time left: 0:18:21\n","Step 82600/104948: Loss=2.3021, Estimated time left: 0:18:17\n","Step 82700/104948: Loss=2.1851, Estimated time left: 0:18:12\n","Step 82800/104948: Loss=2.4755, Estimated time left: 0:18:07\n","Step 82900/104948: Loss=3.3887, Estimated time left: 0:18:02\n","Step 83000/104948: Loss=2.8213, Estimated time left: 0:17:57\n","Step 83000: Loss=2.8213, LR=1.045613e-05, Accuracy=0.2016\n","Step 83100/104948: Loss=2.5072, Estimated time left: 0:17:52\n","Step 83200/104948: Loss=1.7180, Estimated time left: 0:17:47\n","Step 83300/104948: Loss=3.8604, Estimated time left: 0:17:42\n","Step 83400/104948: Loss=3.0134, Estimated time left: 0:17:37\n","Step 83500/104948: Loss=1.7993, Estimated time left: 0:17:32\n","Step 83600/104948: Loss=3.4510, Estimated time left: 0:17:28\n","Step 83700/104948: Loss=1.8520, Estimated time left: 0:17:23\n","Step 83800/104948: Loss=1.0370, Estimated time left: 0:17:18\n","Step 83900/104948: Loss=3.0358, Estimated time left: 0:17:13\n","Step 84000/104948: Loss=2.2692, Estimated time left: 0:17:08\n","Step 84000: Loss=2.2692, LR=9.979704e-06, Accuracy=0.2297\n","Step 84100/104948: Loss=3.5588, Estimated time left: 0:17:03\n","Step 84200/104948: Loss=3.9057, Estimated time left: 0:16:58\n","Step 84300/104948: Loss=2.9179, Estimated time left: 0:16:53\n","Step 84400/104948: Loss=1.9396, Estimated time left: 0:16:48\n","Step 84500/104948: Loss=1.8710, Estimated time left: 0:16:44\n","Step 84600/104948: Loss=3.6765, Estimated time left: 0:16:39\n","Step 84700/104948: Loss=2.1005, Estimated time left: 0:16:34\n","Step 84800/104948: Loss=1.9416, Estimated time left: 0:16:29\n","Step 84900/104948: Loss=2.4329, Estimated time left: 0:16:24\n","Step 85000/104948: Loss=1.8498, Estimated time left: 0:16:19\n","Step 85000: Loss=1.8498, LR=9.503278e-06, Accuracy=0.1862\n","Step 85100/104948: Loss=3.6637, Estimated time left: 0:16:14\n","Step 85200/104948: Loss=2.1101, Estimated time left: 0:16:09\n","Step 85300/104948: Loss=2.3176, Estimated time left: 0:16:04\n","Step 85400/104948: Loss=1.3621, Estimated time left: 0:15:59\n","Step 85500/104948: Loss=2.2782, Estimated time left: 0:15:54\n","Step 85600/104948: Loss=2.7727, Estimated time left: 0:15:50\n","Step 85700/104948: Loss=2.5604, Estimated time left: 0:15:45\n","Step 85800/104948: Loss=3.3327, Estimated time left: 0:15:40\n","Step 85900/104948: Loss=2.1157, Estimated time left: 0:15:35\n","Step 86000/104948: Loss=1.8705, Estimated time left: 0:15:30\n","Step 86000: Loss=1.8705, LR=9.026851e-06, Accuracy=0.2022\n","Step 86100/104948: Loss=2.2339, Estimated time left: 0:15:25\n","Step 86200/104948: Loss=3.2262, Estimated time left: 0:15:20\n","Step 86300/104948: Loss=1.7933, Estimated time left: 0:15:15\n","Step 86400/104948: Loss=3.1141, Estimated time left: 0:15:10\n","Step 86500/104948: Loss=1.4131, Estimated time left: 0:15:06\n","Step 86600/104948: Loss=2.9380, Estimated time left: 0:15:01\n","Step 86700/104948: Loss=2.3449, Estimated time left: 0:14:56\n","Step 86800/104948: Loss=2.9792, Estimated time left: 0:14:51\n","Step 86900/104948: Loss=2.7051, Estimated time left: 0:14:46\n","Step 87000/104948: Loss=2.2682, Estimated time left: 0:14:41\n","Step 87000: Loss=2.2682, LR=8.550425e-06, Accuracy=0.1696\n","Step 87100/104948: Loss=1.2995, Estimated time left: 0:14:36\n","Step 87200/104948: Loss=3.4630, Estimated time left: 0:14:31\n","Step 87300/104948: Loss=2.9244, Estimated time left: 0:14:26\n","Step 87400/104948: Loss=2.4386, Estimated time left: 0:14:21\n","Step 87500/104948: Loss=1.6997, Estimated time left: 0:14:17\n","Step 87600/104948: Loss=3.9247, Estimated time left: 0:14:12\n","Step 87700/104948: Loss=2.5378, Estimated time left: 0:14:07\n","Step 87800/104948: Loss=2.7214, Estimated time left: 0:14:02\n","Step 87900/104948: Loss=3.4273, Estimated time left: 0:13:57\n","Step 88000/104948: Loss=2.9164, Estimated time left: 0:13:52\n","Step 88000: Loss=2.9164, LR=8.073999e-06, Accuracy=0.2146\n","Step 88100/104948: Loss=2.3050, Estimated time left: 0:13:47\n","Step 88200/104948: Loss=2.0123, Estimated time left: 0:13:42\n","Step 88300/104948: Loss=1.1919, Estimated time left: 0:13:37\n","Step 88400/104948: Loss=3.5544, Estimated time left: 0:13:32\n","Step 88500/104948: Loss=2.0729, Estimated time left: 0:13:27\n","Step 88600/104948: Loss=1.3266, Estimated time left: 0:13:23\n","Step 88700/104948: Loss=1.8938, Estimated time left: 0:13:18\n","Step 88800/104948: Loss=3.0670, Estimated time left: 0:13:13\n","Step 88900/104948: Loss=2.8306, Estimated time left: 0:13:08\n","Step 89000/104948: Loss=3.6214, Estimated time left: 0:13:03\n","Step 89000: Loss=3.6214, LR=7.597572e-06, Accuracy=0.1792\n","Step 89100/104948: Loss=1.5344, Estimated time left: 0:12:58\n","Step 89200/104948: Loss=2.5093, Estimated time left: 0:12:53\n","Step 89300/104948: Loss=2.7650, Estimated time left: 0:12:48\n","Step 89400/104948: Loss=2.2146, Estimated time left: 0:12:43\n","Step 89500/104948: Loss=2.7425, Estimated time left: 0:12:38\n","Step 89600/104948: Loss=2.3103, Estimated time left: 0:12:34\n","Step 89700/104948: Loss=2.6079, Estimated time left: 0:12:29\n","Step 89800/104948: Loss=3.9517, Estimated time left: 0:12:24\n","Step 89900/104948: Loss=2.6210, Estimated time left: 0:12:19\n","Step 90000/104948: Loss=4.7737, Estimated time left: 0:12:14\n","Step 90000: Loss=4.7737, LR=7.121146e-06, Accuracy=0.1960\n","Step 90100/104948: Loss=1.7128, Estimated time left: 0:12:09\n","Step 90200/104948: Loss=3.1974, Estimated time left: 0:12:04\n","Step 90300/104948: Loss=4.6889, Estimated time left: 0:11:59\n","Step 90400/104948: Loss=2.5884, Estimated time left: 0:11:54\n","Step 90500/104948: Loss=2.6536, Estimated time left: 0:11:49\n","Step 90600/104948: Loss=3.1139, Estimated time left: 0:11:44\n","Step 90700/104948: Loss=3.4624, Estimated time left: 0:11:40\n","Step 90800/104948: Loss=4.1267, Estimated time left: 0:11:35\n","Step 90900/104948: Loss=2.7147, Estimated time left: 0:11:30\n","Step 91000/104948: Loss=2.5999, Estimated time left: 0:11:25\n","Step 91000: Loss=2.5999, LR=6.644719e-06, Accuracy=0.2060\n","Step 91100/104948: Loss=2.0153, Estimated time left: 0:11:20\n","Step 91200/104948: Loss=2.5450, Estimated time left: 0:11:15\n","Step 91300/104948: Loss=2.4023, Estimated time left: 0:11:10\n","Step 91400/104948: Loss=1.3204, Estimated time left: 0:11:05\n","Step 91500/104948: Loss=3.2088, Estimated time left: 0:11:00\n","Step 91600/104948: Loss=1.4380, Estimated time left: 0:10:55\n","Step 91700/104948: Loss=1.9025, Estimated time left: 0:10:51\n","Step 91800/104948: Loss=2.2838, Estimated time left: 0:10:46\n","Step 91900/104948: Loss=1.8035, Estimated time left: 0:10:41\n","Step 92000/104948: Loss=1.7310, Estimated time left: 0:10:36\n","Step 92000: Loss=1.7310, LR=6.168293e-06, Accuracy=0.1898\n","Step 92100/104948: Loss=2.3630, Estimated time left: 0:10:31\n","Step 92200/104948: Loss=2.7658, Estimated time left: 0:10:26\n","Step 92300/104948: Loss=3.0435, Estimated time left: 0:10:21\n","Step 92400/104948: Loss=2.2438, Estimated time left: 0:10:16\n","Step 92500/104948: Loss=4.2430, Estimated time left: 0:10:11\n","Step 92600/104948: Loss=2.5886, Estimated time left: 0:10:06\n","Step 92700/104948: Loss=2.0685, Estimated time left: 0:10:01\n","Step 92800/104948: Loss=3.1232, Estimated time left: 0:09:57\n","Step 92900/104948: Loss=2.3082, Estimated time left: 0:09:52\n","Step 93000/104948: Loss=2.3046, Estimated time left: 0:09:47\n","Step 93000: Loss=2.3046, LR=5.691866e-06, Accuracy=0.1612\n","Step 93100/104948: Loss=4.1026, Estimated time left: 0:09:42\n","Step 93200/104948: Loss=2.7018, Estimated time left: 0:09:37\n","Step 93300/104948: Loss=2.9731, Estimated time left: 0:09:32\n","Step 93400/104948: Loss=2.9475, Estimated time left: 0:09:27\n","Step 93500/104948: Loss=2.7827, Estimated time left: 0:09:22\n","Step 93600/104948: Loss=2.4232, Estimated time left: 0:09:17\n","Step 93700/104948: Loss=2.0234, Estimated time left: 0:09:12\n","Step 93800/104948: Loss=1.9205, Estimated time left: 0:09:07\n","Step 93900/104948: Loss=2.6728, Estimated time left: 0:09:03\n","Step 94000/104948: Loss=3.1658, Estimated time left: 0:08:58\n","Step 94000: Loss=3.1658, LR=5.215440e-06, Accuracy=0.2112\n","Step 94100/104948: Loss=1.8189, Estimated time left: 0:08:53\n","Step 94200/104948: Loss=4.0051, Estimated time left: 0:08:48\n","Step 94300/104948: Loss=3.3710, Estimated time left: 0:08:43\n","Step 94400/104948: Loss=1.4083, Estimated time left: 0:08:38\n","Step 94500/104948: Loss=2.8737, Estimated time left: 0:08:33\n","Step 94600/104948: Loss=2.8351, Estimated time left: 0:08:28\n","Step 94700/104948: Loss=2.8471, Estimated time left: 0:08:23\n","Step 94800/104948: Loss=2.4105, Estimated time left: 0:08:18\n","Step 94900/104948: Loss=2.4111, Estimated time left: 0:08:13\n","Step 95000/104948: Loss=3.6078, Estimated time left: 0:08:09\n","Step 95000: Loss=3.6078, LR=4.739014e-06, Accuracy=0.2033\n","Step 95100/104948: Loss=2.1485, Estimated time left: 0:08:04\n","Step 95200/104948: Loss=1.7995, Estimated time left: 0:07:59\n","Step 95300/104948: Loss=2.2575, Estimated time left: 0:07:54\n","Step 95400/104948: Loss=3.1263, Estimated time left: 0:07:49\n","Step 95500/104948: Loss=1.8991, Estimated time left: 0:07:44\n","Step 95600/104948: Loss=2.6922, Estimated time left: 0:07:39\n","Step 95700/104948: Loss=3.3823, Estimated time left: 0:07:34\n","Step 95800/104948: Loss=2.2676, Estimated time left: 0:07:29\n","Step 95900/104948: Loss=3.2434, Estimated time left: 0:07:24\n","Step 96000/104948: Loss=3.7601, Estimated time left: 0:07:19\n","Step 96000: Loss=3.7601, LR=4.262587e-06, Accuracy=0.2139\n","Step 96100/104948: Loss=1.6784, Estimated time left: 0:07:15\n","Step 96200/104948: Loss=2.0498, Estimated time left: 0:07:10\n","Step 96300/104948: Loss=1.6448, Estimated time left: 0:07:05\n","Step 96400/104948: Loss=2.6617, Estimated time left: 0:07:00\n","Step 96500/104948: Loss=2.0435, Estimated time left: 0:06:55\n","Step 96600/104948: Loss=2.5896, Estimated time left: 0:06:50\n","Step 96700/104948: Loss=2.1869, Estimated time left: 0:06:45\n","Step 96800/104948: Loss=1.9779, Estimated time left: 0:06:40\n","Step 96900/104948: Loss=2.4369, Estimated time left: 0:06:35\n","Step 97000/104948: Loss=3.5850, Estimated time left: 0:06:30\n","Step 97000: Loss=3.5850, LR=3.786161e-06, Accuracy=0.1989\n","Step 97100/104948: Loss=2.6278, Estimated time left: 0:06:25\n","Step 97200/104948: Loss=1.1113, Estimated time left: 0:06:20\n","Step 97300/104948: Loss=2.8575, Estimated time left: 0:06:16\n","Step 97400/104948: Loss=2.0462, Estimated time left: 0:06:11\n","Step 97500/104948: Loss=1.8861, Estimated time left: 0:06:06\n","Step 97600/104948: Loss=2.8539, Estimated time left: 0:06:01\n","Step 97700/104948: Loss=3.2289, Estimated time left: 0:05:56\n","Step 97800/104948: Loss=2.1573, Estimated time left: 0:05:51\n","Step 97900/104948: Loss=1.8367, Estimated time left: 0:05:46\n","Step 98000/104948: Loss=1.5560, Estimated time left: 0:05:41\n","Step 98000: Loss=1.5560, LR=3.309734e-06, Accuracy=0.2228\n","Step 98100/104948: Loss=2.9448, Estimated time left: 0:05:36\n","Step 98200/104948: Loss=2.8601, Estimated time left: 0:05:31\n","Step 98300/104948: Loss=2.1608, Estimated time left: 0:05:26\n","Step 98400/104948: Loss=1.1607, Estimated time left: 0:05:22\n","Step 98500/104948: Loss=2.4589, Estimated time left: 0:05:17\n","Step 98600/104948: Loss=2.8018, Estimated time left: 0:05:12\n","Step 98700/104948: Loss=2.9629, Estimated time left: 0:05:07\n","Step 98800/104948: Loss=2.0001, Estimated time left: 0:05:02\n","Step 98900/104948: Loss=2.7342, Estimated time left: 0:04:57\n","Step 99000/104948: Loss=3.3813, Estimated time left: 0:04:52\n","Step 99000: Loss=3.3813, LR=2.833308e-06, Accuracy=0.2064\n","Step 99100/104948: Loss=2.6938, Estimated time left: 0:04:47\n","Step 99200/104948: Loss=3.2860, Estimated time left: 0:04:42\n","Step 99300/104948: Loss=1.8346, Estimated time left: 0:04:37\n","Step 99400/104948: Loss=2.6996, Estimated time left: 0:04:32\n","Step 99500/104948: Loss=2.5817, Estimated time left: 0:04:27\n","Step 99600/104948: Loss=2.6086, Estimated time left: 0:04:23\n","Step 99700/104948: Loss=2.1711, Estimated time left: 0:04:18\n","Step 99800/104948: Loss=3.1712, Estimated time left: 0:04:13\n","Step 99900/104948: Loss=2.4422, Estimated time left: 0:04:08\n","Step 100000/104948: Loss=1.2181, Estimated time left: 0:04:03\n","Step 100000: Loss=1.2181, LR=2.356882e-06, Accuracy=0.1730\n","Step 100100/104948: Loss=1.4778, Estimated time left: 0:03:58\n","Step 100200/104948: Loss=2.8127, Estimated time left: 0:03:53\n","Step 100300/104948: Loss=2.3686, Estimated time left: 0:03:48\n","Step 100400/104948: Loss=2.1395, Estimated time left: 0:03:43\n","Step 100500/104948: Loss=2.1217, Estimated time left: 0:03:38\n","Step 100600/104948: Loss=2.9544, Estimated time left: 0:03:33\n","Step 100700/104948: Loss=2.6063, Estimated time left: 0:03:28\n","Step 100800/104948: Loss=2.6421, Estimated time left: 0:03:23\n","Step 100900/104948: Loss=2.4010, Estimated time left: 0:03:19\n","Step 101000/104948: Loss=2.9410, Estimated time left: 0:03:14\n","Step 101000: Loss=2.9410, LR=1.880455e-06, Accuracy=0.2068\n","Step 101100/104948: Loss=3.0763, Estimated time left: 0:03:09\n","Step 101200/104948: Loss=2.7423, Estimated time left: 0:03:04\n","Step 101300/104948: Loss=2.5739, Estimated time left: 0:02:59\n","Step 101400/104948: Loss=2.8107, Estimated time left: 0:02:54\n","Step 101500/104948: Loss=2.7348, Estimated time left: 0:02:49\n","Step 101600/104948: Loss=1.5266, Estimated time left: 0:02:44\n","Step 101700/104948: Loss=3.4460, Estimated time left: 0:02:39\n","Step 101800/104948: Loss=1.8880, Estimated time left: 0:02:34\n","Step 101900/104948: Loss=3.2053, Estimated time left: 0:02:29\n","Step 102000/104948: Loss=2.5790, Estimated time left: 0:02:24\n","Step 102000: Loss=2.5790, LR=1.404029e-06, Accuracy=0.1911\n","Step 102100/104948: Loss=2.5451, Estimated time left: 0:02:20\n","Step 102200/104948: Loss=2.3044, Estimated time left: 0:02:15\n","Step 102300/104948: Loss=3.9044, Estimated time left: 0:02:10\n","Step 102400/104948: Loss=4.4573, Estimated time left: 0:02:05\n","Step 102500/104948: Loss=4.3857, Estimated time left: 0:02:00\n","Step 102600/104948: Loss=1.9944, Estimated time left: 0:01:55\n","Step 102700/104948: Loss=2.0383, Estimated time left: 0:01:50\n","Step 102800/104948: Loss=2.5229, Estimated time left: 0:01:45\n","Step 102900/104948: Loss=2.8361, Estimated time left: 0:01:40\n","Step 103000/104948: Loss=3.5198, Estimated time left: 0:01:35\n","Step 103000: Loss=3.5198, LR=9.276022e-07, Accuracy=0.2081\n","Step 103100/104948: Loss=1.1764, Estimated time left: 0:01:30\n","Step 103200/104948: Loss=3.6025, Estimated time left: 0:01:25\n","Step 103300/104948: Loss=1.3769, Estimated time left: 0:01:21\n","Step 103400/104948: Loss=2.0855, Estimated time left: 0:01:16\n","Step 103500/104948: Loss=3.9269, Estimated time left: 0:01:11\n","Step 103600/104948: Loss=2.8026, Estimated time left: 0:01:06\n","Step 103700/104948: Loss=2.5227, Estimated time left: 0:01:01\n","Step 103800/104948: Loss=2.4718, Estimated time left: 0:00:56\n","Step 103900/104948: Loss=2.8068, Estimated time left: 0:00:51\n","Step 104000/104948: Loss=2.2303, Estimated time left: 0:00:46\n","Step 104000: Loss=2.2303, LR=4.511758e-07, Accuracy=0.2129\n","Step 104100/104948: Loss=2.3194, Estimated time left: 0:00:41\n","Step 104200/104948: Loss=2.7998, Estimated time left: 0:00:36\n","Step 104300/104948: Loss=3.3755, Estimated time left: 0:00:31\n","Step 104400/104948: Loss=1.0695, Estimated time left: 0:00:26\n","Step 104500/104948: Loss=3.4566, Estimated time left: 0:00:21\n","Step 104600/104948: Loss=2.5336, Estimated time left: 0:00:17\n","Step 104700/104948: Loss=2.7470, Estimated time left: 0:00:12\n","Step 104800/104948: Loss=1.2998, Estimated time left: 0:00:07\n","Step 104900/104948: Loss=1.8627, Estimated time left: 0:00:02\n","Model saved as '/content/drive/My Drive/Colab Notebooks/DS 5690/final_project/poetic-gpt-chinese/save.model.pth'\n"]}],"source":["import torch\n","from torch.cuda.amp import GradScaler, autocast\n","from transformers import AdamW\n","from transformers.optimization import get_scheduler\n","import datetime\n","import time\n","\n","# Define the training function with Mixed Precision and Time Estimation\n","def train_and_save():\n","    \"\"\"\n","    Train the model using Mixed Precision Training, log progress, and save the model.\n","    Includes:\n","    - Logging metrics every 1000 steps.\n","    - Estimated time remaining for training.\n","    - Mixed Precision Training with GradScaler and autocast.\n","    \"\"\"\n","    global model  # Use the global model variable\n","    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","    model = model.to(device)\n","\n","    # Initialize the optimizer and scheduler\n","    optimizer = AdamW(model.parameters(), lr=5e-5)\n","    scheduler = get_scheduler(\n","        name='linear',\n","        num_warmup_steps=0,\n","        num_training_steps=len(loader),\n","        optimizer=optimizer\n","    )\n","\n","    # Mixed Precision: Initialize GradScaler\n","    scaler = GradScaler()\n","\n","    # Set the model to training mode\n","    model.train()\n","\n","    # Get the total number of batches\n","    total_batches = len(loader)\n","\n","    # Start timing\n","    start_time = time.time()\n","\n","    # Iterate through the DataLoader\n","    for i, data in enumerate(loader):\n","        # Move data to the appropriate device\n","        for k in data.keys():\n","            data[k] = data[k].to(device)\n","\n","        # Mixed Precision: Forward pass with autocast\n","        with autocast():\n","            out = model(**data)\n","            loss = out['loss']\n","\n","        # Backward pass with GradScaler\n","        scaler.scale(loss).backward()\n","\n","        # Gradient clipping\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # Optimizer step with GradScaler\n","        scaler.step(optimizer)\n","        scaler.update()\n","\n","        # Scheduler step\n","        scheduler.step()\n","\n","        # Zero gradients for the next step\n","        optimizer.zero_grad()\n","        model.zero_grad()\n","\n","        # Estimate time remaining every 100 steps\n","        if i % 100 == 0:\n","            elapsed_time = time.time() - start_time\n","            avg_time_per_batch = elapsed_time / (i + 1)\n","            remaining_batches = total_batches - (i + 1)\n","            estimated_time_left = remaining_batches * avg_time_per_batch\n","            formatted_time_left = str(datetime.timedelta(seconds=int(estimated_time_left)))\n","\n","            # Log progress\n","            print(f\"Step {i}/{total_batches}: Loss={loss.item():.4f}, Estimated time left: {formatted_time_left}\")\n","\n","        # Log metrics every 1000 steps\n","        if i % 1000 == 0:\n","            labels = data['labels'][:, 1:]  # Shifted true labels\n","            out = out['logits'].argmax(dim=2)[:, :-1]  # Predicted labels\n","\n","            # Mask padding tokens for accuracy calculation\n","            select = labels != 0\n","            labels = labels[select]\n","            out = out[select]\n","\n","            # Calculate accuracy\n","            accuracy = (labels == out).sum().item() / labels.numel()\n","\n","            # Get the current learning rate\n","            lr = optimizer.state_dict()['param_groups'][0]['lr']\n","\n","            # Log the current metrics\n","            print(f\"Step {i}: Loss={loss.item():.4f}, LR={lr:.6e}, Accuracy={accuracy:.4f}\")\n","\n","    # Save the model\n","    model = model.to('cpu')  # Move the model to CPU for saving\n","    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")  # Add a timestamp\n","    save_path = os.path.join(BASE_DIR, f\"save.model.pth\")\n","    torch.save(model.state_dict(), save_path)  # Save the state_dict\n","\n","    print(f\"Model saved as '{save_path}'\")\n","\n","# Call the training function\n","train_and_save()\n"]},{"cell_type":"code","source":["model = torch.load(model_path)\n","\n"],"metadata":{"id":"9H7FYeze23tF","executionInfo":{"status":"ok","timestamp":1733446783280,"user_tz":360,"elapsed":1297,"user":{"displayName":"Jiayi Xu","userId":"16580075947723802783"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"eb241b43-f5e2-44c3-f969-6b921c6b1f4b"},"id":"9H7FYeze23tF","execution_count":15,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-15-216a83eaf066>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model = torch.load(model_path)\n"]}]},{"cell_type":"code","source":["!pip install gradio"],"metadata":{"id":"dqnFgKwM24ZL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733446790759,"user_tz":360,"elapsed":2729,"user":{"displayName":"Jiayi Xu","userId":"16580075947723802783"}},"outputId":"5a3da6de-00a1-4ded-a2e4-ca8bcdf07234"},"id":"dqnFgKwM24ZL","execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (5.8.0)\n","Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n","Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n","Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.115.6)\n","Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.4.0)\n","Requirement already satisfied: gradio-client==1.5.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.1)\n","Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.28.0)\n","Requirement already satisfied: huggingface-hub>=0.25.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.26.3)\n","Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n","Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n","Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n","Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.12)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.2)\n","Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n","Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (11.0.0)\n","Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.2)\n","Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n","Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.19)\n","Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n","Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.8.2)\n","Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.1.6)\n","Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n","Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.41.3)\n","Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.13.2)\n","Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.14.0)\n","Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n","Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.32.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.1->gradio) (2024.10.0)\n","Requirement already satisfied: websockets<15.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.1->gradio) (14.1)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (3.16.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (4.66.6)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.27.1)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (3.4.0)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (2.2.3)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"]}]},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM\n","\n","# Define the model architecture\n","model = AutoModelForCausalLM.from_pretrained('uer/gpt2-chinese-cluecorpussmall')\n","\n","# Load the weights into the model\n","model.load_state_dict(torch.load(model_path))\n","model.eval()  # Set to evaluation mode\n","\n","\n","# Test the model by generating text\n","generate(\"秋日\", row=4, col=5)"],"metadata":{"id":"s4zE_lf33kJR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733448867977,"user_tz":360,"elapsed":3274,"user":{"displayName":"Jiayi Xu","userId":"16580075947723802783"}},"outputId":"8e4425e1-cab9-438e-f13a-9a01cead7ea4"},"id":"s4zE_lf33kJR","execution_count":41,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-41-d1c64fe17952>:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(model_path))\n"]},{"output_type":"stream","name":"stdout","text":["0 [CLS] 秋 日 明 月 无 ， 春 风 有 旧 阳 。 春 来 无 限 好 ， 日 已 无 尽 新 。\n","1 [CLS] 秋 日 何 处 闻 ， 松 篱 松 梢 见 。 不 识 春 香 草 ， 深 深 见 草 时 。\n","2 [CLS] 秋 日 清 流 多 ， 月 里 江 水 清 。 何 须 人 作 山 ， 亦 自 东 山 有 。\n"]}]},{"cell_type":"code","source":["import io\n","import sys\n","import gradio as gr\n","\n","# Wrapper function for Gradio to capture printed output\n","def gradio_generate(seed_text, rows, cols):\n","    \"\"\"\n","    Wrapper for the generate function to use with Gradio.\n","    Captures the printed output and returns it as a string.\n","    \"\"\"\n","    # Redirect standard output to capture prints\n","    old_stdout = sys.stdout\n","    new_stdout = io.StringIO()\n","    sys.stdout = new_stdout\n","\n","    # Call the original generate function\n","    generate(seed_text, row=int(rows), col=int(cols))\n","\n","    # Restore standard output and fetch printed content\n","    sys.stdout = old_stdout\n","    output = new_stdout.getvalue()\n","\n","    return output\n","\n","# Gradio interface\n","interface = gr.Interface(\n","    fn=gradio_generate,  # Use the wrapper function\n","    inputs=[\n","        gr.Textbox(lines=2, label=\"Seed Text\", placeholder=\"Enter the seed text for the poem...\"),\n","        gr.Number(label=\"Rows\", value=4, precision=0),\n","        gr.Number(label=\"Columns\", value=5, precision=0),\n","    ],\n","    outputs=\"text\",\n","    title=\"Interactive Poem Generator\",\n","    description=\"Enter a seed text and customize the structure (rows and columns). The model will generate a structured Chinese poem.\"\n",")\n","\n","# Launch the Gradio interface\n","interface.launch()\n"],"metadata":{"id":"QGn9qsBr3nDn","colab":{"base_uri":"https://localhost:8080/","height":648},"executionInfo":{"status":"ok","timestamp":1733448876842,"user_tz":360,"elapsed":2601,"user":{"displayName":"Jiayi Xu","userId":"16580075947723802783"}},"outputId":"4077fde4-b747-4773-b6b6-3879e266067c"},"id":"QGn9qsBr3nDn","execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","* Running on public URL: https://5c1eeadbc2c5867b6a.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://5c1eeadbc2c5867b6a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":42}]},{"cell_type":"code","source":[],"metadata":{"id":"dPOdDM_dgNXc"},"id":"dPOdDM_dgNXc","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"},"colab":{"provenance":[],"gpuType":"A100"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"e40023312080459cb865b8fca92cecfe":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_916a79fe39364609a2b90fd5f5021c5c","IPY_MODEL_0d49928344ef474abf6ebfc5aa83060b","IPY_MODEL_bd58e8dad06047a0b11555ecedd691f1"],"layout":"IPY_MODEL_499d4db1ee8248a1a8a25238c7ae88d8"}},"916a79fe39364609a2b90fd5f5021c5c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eb9faa893fa142fba2310253def0ac2f","placeholder":"​","style":"IPY_MODEL_215733bca9e34722abb8d842edd72dc7","value":"tokenizer_config.json: 100%"}},"0d49928344ef474abf6ebfc5aa83060b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7f71001f77b04f31922667d0e2e67926","max":217,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ef46ad44ca5042e688063ea51cfc5795","value":217}},"bd58e8dad06047a0b11555ecedd691f1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_018338a570364f71aa499798ea281a56","placeholder":"​","style":"IPY_MODEL_c156154b9b24418da25fb96e36399533","value":" 217/217 [00:00&lt;00:00, 18.3kB/s]"}},"499d4db1ee8248a1a8a25238c7ae88d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb9faa893fa142fba2310253def0ac2f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"215733bca9e34722abb8d842edd72dc7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7f71001f77b04f31922667d0e2e67926":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef46ad44ca5042e688063ea51cfc5795":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"018338a570364f71aa499798ea281a56":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c156154b9b24418da25fb96e36399533":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"92eea51ef67644eb9ed9362dab572989":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_04224baa60374bf9b23a5e195e77dd68","IPY_MODEL_854db283180e4114b73493a02b124ecb","IPY_MODEL_9866926ea9ef4edead8b8e70b6e27dcd"],"layout":"IPY_MODEL_a2400c9ba01f4c1c8869119cae98a25d"}},"04224baa60374bf9b23a5e195e77dd68":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5fee88f4552c4093bd597a458df85e84","placeholder":"​","style":"IPY_MODEL_d2ebd47d17b24e9b8bfda9e2a3ed6b4e","value":"config.json: 100%"}},"854db283180e4114b73493a02b124ecb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c75e1539067843db85bb5a88e2d931ca","max":577,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1c7207488fbb46e4930dd843c408a06c","value":577}},"9866926ea9ef4edead8b8e70b6e27dcd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_68cf11a3631c479f97ae1c6dae0a1151","placeholder":"​","style":"IPY_MODEL_a7ef61767a034437b4078407f37ff51f","value":" 577/577 [00:00&lt;00:00, 46.4kB/s]"}},"a2400c9ba01f4c1c8869119cae98a25d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5fee88f4552c4093bd597a458df85e84":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d2ebd47d17b24e9b8bfda9e2a3ed6b4e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c75e1539067843db85bb5a88e2d931ca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c7207488fbb46e4930dd843c408a06c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"68cf11a3631c479f97ae1c6dae0a1151":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a7ef61767a034437b4078407f37ff51f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1765aa38f57444678d08e98c91785acf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d3b9d228867544229a255643f93a2ffe","IPY_MODEL_0cd7110ded914090a8bf86f85996905b","IPY_MODEL_0fb70d6d76d848958b32ee72708f66e5"],"layout":"IPY_MODEL_ff8bd7978b134fd7b0847a7ce55442d8"}},"d3b9d228867544229a255643f93a2ffe":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_64fb94c83ba8477b8151bf89366a6761","placeholder":"​","style":"IPY_MODEL_62074c8b785b464aa9955ad492588686","value":"vocab.txt: 100%"}},"0cd7110ded914090a8bf86f85996905b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9923c6308b6946b698e1ec25c2d1f89c","max":109540,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e677d925f52b453b9d6491a64337a6fa","value":109540}},"0fb70d6d76d848958b32ee72708f66e5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7cc3097118e646ec85fc558ecd0ba2f5","placeholder":"​","style":"IPY_MODEL_98997ceafa234a17aa204b571773a1d6","value":" 110k/110k [00:00&lt;00:00, 7.65MB/s]"}},"ff8bd7978b134fd7b0847a7ce55442d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"64fb94c83ba8477b8151bf89366a6761":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"62074c8b785b464aa9955ad492588686":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9923c6308b6946b698e1ec25c2d1f89c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e677d925f52b453b9d6491a64337a6fa":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7cc3097118e646ec85fc558ecd0ba2f5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"98997ceafa234a17aa204b571773a1d6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2201dcb87dd949819d993c5575570d9d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6b6225b65fed43f99cd32f12aad7d508","IPY_MODEL_15a277a8765749ec9fafda5f0304716c","IPY_MODEL_d6b2a9ca1a6449bba40925e3d0242b02"],"layout":"IPY_MODEL_4e3af6bbc7da4cadace59fe7850c2e8b"}},"6b6225b65fed43f99cd32f12aad7d508":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1dc8e23b4b834cd28fb4210dde25c279","placeholder":"​","style":"IPY_MODEL_da4e1a38276340f7962641da5aa64db5","value":"special_tokens_map.json: 100%"}},"15a277a8765749ec9fafda5f0304716c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9341dfcb6eba411fad6b7e9c86e3d9d3","max":112,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7041e0f3d0f84450b7414a8838199054","value":112}},"d6b2a9ca1a6449bba40925e3d0242b02":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3864547d13cb42f2b64175d2e8623606","placeholder":"​","style":"IPY_MODEL_330218db988b4ff58359ce21543cde60","value":" 112/112 [00:00&lt;00:00, 9.91kB/s]"}},"4e3af6bbc7da4cadace59fe7850c2e8b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1dc8e23b4b834cd28fb4210dde25c279":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"da4e1a38276340f7962641da5aa64db5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9341dfcb6eba411fad6b7e9c86e3d9d3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7041e0f3d0f84450b7414a8838199054":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3864547d13cb42f2b64175d2e8623606":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"330218db988b4ff58359ce21543cde60":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"17d1b792c6e04f1db222e8cebc81b778":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_375f9ecb32004e298e6fb3c02a33f35a","IPY_MODEL_da3c5198e1a0429cb3b54050a3baf78d","IPY_MODEL_4f67444df3614a7898f81bf09fa1add4"],"layout":"IPY_MODEL_ac8d1459bfb443f9be0050fa32d04901"}},"375f9ecb32004e298e6fb3c02a33f35a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_144942b05f0e440d9afbe2a9b5c6ec0b","placeholder":"​","style":"IPY_MODEL_f74a08037dc244878205bf53b8be64f6","value":"pytorch_model.bin: 100%"}},"da3c5198e1a0429cb3b54050a3baf78d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3f59be421a5d436ba748f321b3fc5b58","max":420921295,"min":0,"orientation":"horizontal","style":"IPY_MODEL_46fa8ca4e372405295f0f0f4b229f401","value":420921295}},"4f67444df3614a7898f81bf09fa1add4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0f74736332d04f68b20dc05279735d6a","placeholder":"​","style":"IPY_MODEL_d8a12e1bdf7242a0a0d79c3f2e17a058","value":" 421M/421M [00:01&lt;00:00, 258MB/s]"}},"ac8d1459bfb443f9be0050fa32d04901":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"144942b05f0e440d9afbe2a9b5c6ec0b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f74a08037dc244878205bf53b8be64f6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3f59be421a5d436ba748f321b3fc5b58":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"46fa8ca4e372405295f0f0f4b229f401":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0f74736332d04f68b20dc05279735d6a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d8a12e1bdf7242a0a0d79c3f2e17a058":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}